{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "import bleach\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from ml_project_2_mlp import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load categories.json from data/crowdsourced\n",
    "with open(\"../data/crowdsourced/raw/categories.json\") as f:\n",
    "    categories = json.load(f)\n",
    "\n",
    "# Load labeled.csv from data/crowdsourced\n",
    "labeled = pd.read_csv(\"../data/crowdsourced/raw/labeled.csv\")\n",
    "\n",
    "# Load the website content from data/crowdsourced from pickle\n",
    "with open(\"../data/crowdsourced/raw/content.pkl\", \"rb\") as f:\n",
    "    content = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crowdsourced Data: Labeling Analysis\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with some basic EDA:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Task title: {labeled['Title'][0]}\")\n",
    "print(f\"Task description: {labeled['Description'][0]}\")\n",
    "print(f\"Task reward: {labeled['Reward'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of records per unique page\n",
    "number_of_labels, count = np.unique(\n",
    "    labeled[\"Input.uid\"].value_counts(), return_counts=True\n",
    ")\n",
    "for numlabels, c in zip(number_of_labels, count):\n",
    "    print(f\"There are {c} websites each annotated by {numlabels} labelers\")\n",
    "\n",
    "# Show the unique responses for each question\n",
    "answers = set()\n",
    "for answer in labeled[\"Answer.taskAnswers\"]:\n",
    "    parsed_answer = json.loads(answer)\n",
    "    answers.update([v for v in parsed_answer[0].values() if type(v) == str])\n",
    "print(f\"There are {len(answers)} unique responses: {answers}\")\n",
    "\n",
    "# Average number of labels per user\n",
    "avg_user_labels = labeled[\"WorkerId\"].value_counts().mean()\n",
    "print(f\"On average each labeler annotated {avg_user_labels} pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that for all records, AssignmentStatus is Approved\n",
    "assert (\n",
    "    len(labeled[\"AssignmentStatus\"].unique()) == 1\n",
    "    and labeled[\"AssignmentStatus\"].unique()[0] == \"Approved\"\n",
    "), \"AssignmentStatus is not Approved\"\n",
    "print(\"✅ All records have AssignmentStatus Approved\")\n",
    "\n",
    "# Confirm that all pages are assigned with at most 3 assignments\n",
    "max_assignments, count = np.unique(\n",
    "    labeled[\"MaxAssignments\"], return_counts=True)\n",
    "assert len(\n",
    "    max_assignments) == 1 and max_assignments[0] == 3, \"MaxAssignments is not 3\"\n",
    "print(\"✅ This checks with the max assignments allowed.\")\n",
    "\n",
    "\n",
    "# Get Double Check that the list in TaskAnswers is of length 1 always\n",
    "answers = set()\n",
    "total = 0\n",
    "for answer in labeled[\"Answer.taskAnswers\"]:\n",
    "    parsed_answer = json.loads(answer)\n",
    "    if len(parsed_answer) > 1:\n",
    "        total += 1\n",
    "if total > 0:\n",
    "    print(f\"❗️ There are {total} records with taskAnswers list length > 1\")\n",
    "else:\n",
    "    print(\"✅ All records has taskAnswers list length = 1\")\n",
    "\n",
    "# Check missing values for Input.url, Input.screenshot, Input.title, Input.description, report in percentage\n",
    "for col in [\"Input.url\", \"Input.screenshot\", \"Input.title\", \"Input.description\"]:\n",
    "    miss_vals = labeled[col].isna().sum() / len(labeled) * 100\n",
    "    if miss_vals > 0:\n",
    "        print(f\"❗️ {col} has {miss_vals:.2f}% missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's one hot encode the column `Answer.taskAnswers` based on the\n",
    "dictionary that each row includes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create idx2cat and cat2idx mappings\n",
    "idx2cat = {idx: categories[idx][\"name\"] for idx in categories}\n",
    "cat2idx = {categories[idx][\"name\"]: idx for idx in categories}\n",
    "\n",
    "# Create a new column AnswersParsed\n",
    "labeled[\"AnswersParsed\"] = labeled[\"Answer.taskAnswers\"].apply(\n",
    "    lambda x: {\n",
    "        k.split(\"-\")[-1]: v for k, v in json.loads(x)[0].items() if type(v) == str\n",
    "    }\n",
    ")\n",
    "\n",
    "# Obtain the selected Idx and corresponding categories\n",
    "labeled[\"SelectedIdx\"] = labeled[\"AnswersParsed\"].apply(\n",
    "    lambda x: [k for k, v in x.items() if v == \"YES\"]\n",
    ")\n",
    "labeled[\"SelectedCategories\"] = labeled[\"SelectedIdx\"].apply(\n",
    "    lambda x: [idx2cat[idx] for idx in x]\n",
    ")\n",
    "\n",
    "# Now, let's one hot encode the selected categories\n",
    "for cat in cat2idx:\n",
    "    labeled[cat] = labeled[\"SelectedCategories\"].apply(\n",
    "        lambda x: 1 if cat in x else 0)\n",
    "\n",
    "relevant_columns = [\"Input.uid\", \"Input.url\"] + list(cat2idx)\n",
    "labeled = labeled[relevant_columns]\n",
    "labeled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for each website and category, we want to look at the aggreement accross\n",
    "annotators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_ids = labeled[\"Input.uid\"].unique()\n",
    "aggrements = []\n",
    "for wid in website_ids:\n",
    "    # Get all the annotations for this website\n",
    "    annotations = labeled[labeled[\"Input.uid\"] == wid].iloc[:, 2:].to_numpy()\n",
    "\n",
    "    # Pair the annotations\n",
    "    kappas = []\n",
    "    for i in range(len(annotations)):\n",
    "        for j in range(i + 1, len(annotations)):\n",
    "            kappas.append(cohen_kappa_score(annotations[i], annotations[j]))\n",
    "\n",
    "    # Take the average of all the kappas\n",
    "    avg_kappa = np.mean(kappas)\n",
    "\n",
    "    # If nan, then set to 0\n",
    "    if np.isnan(avg_kappa):\n",
    "        avg_kappa = 0\n",
    "\n",
    "    # Save the average kappa for this website\n",
    "    aggrements.append([wid, avg_kappa])\n",
    "\n",
    "# Turn into pandas dataframe\n",
    "aggrements = pd.DataFrame(aggrements, columns=[\"Input.uid\", \"Aggrement\"])\n",
    "\n",
    "# Plot the distribution of aggrements\n",
    "sns.histplot(aggrements[\"Aggrement\"])\n",
    "plt.title(\n",
    "    \"Distribution of Aggrements with mean = {:.2f}\".format(\n",
    "        aggrements[\"Aggrement\"].mean()\n",
    "    )\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's use different aggregation strategies to obtain final labels for each\n",
    "webpage:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute for each website the number of times the website was assigned given label\n",
    "page_labc = labeled.groupby(\"Input.uid\").sum()\n",
    "\n",
    "# For each website, decided whether it belongs to the category or not based on the threshold = min. number of annotations\n",
    "thresholds = [1, 2, 3]\n",
    "thresholded = []\n",
    "for t in thresholds:\n",
    "    thresholded.append((page_labc.iloc[:, 3:] >= t).astype(int))\n",
    "\n",
    "# Show the distribution of the number of categories per website\n",
    "numlab_dist = [thresholded[t - 1].sum(axis=1) for t in thresholds]\n",
    "print(\n",
    "    f\"For threshold = 1, the mean number of categories per website is {numlab_dist[0].mean():.2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"For threshold = 2, the mean number of categories per website is {numlab_dist[1].mean():.2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"For threshold = 3, the mean number of categories per website is {numlab_dist[2].mean():.2f}\"\n",
    ")\n",
    "\n",
    "# Print the distribution of the number of categories per website\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for i, ax in enumerate(axs):\n",
    "    sns.barplot(\n",
    "        x=numlab_dist[i].value_counts().index,\n",
    "        y=numlab_dist[i].value_counts().values / numlab_dist[i].shape[0],\n",
    "        ax=ax,\n",
    "        color=\"#31748f\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crowdsourced Data: Content Analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we look closer into what the annotators actually had to label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the list of dict into a dataframe\n",
    "content_df = pd.DataFrame(content)\n",
    "\n",
    "# Fill Nan in is_valid with False\n",
    "content_df[\"is_valid\"].fillna(False, inplace=True)\n",
    "\n",
    "content_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assess how well we were able to parse the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ℹ️ Number of websites in content_df: {content_df.shape[0]}\")\n",
    "print(f\"ℹ️ Number of websites with http status code 200: {content_df[content_df['http_code'] == 200].shape[0]}\")\n",
    "print(f\"ℹ️ Number of websites with valid websites {content_df[content_df['is_valid']].shape[0]}\")\n",
    "print(f\"ℹ️ Number of websites with redirect: {content_df[content_df['redirect_url'].notna()].shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's dig deeper, first, we start with the distribution of the http codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the figure\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "# Without 200\n",
    "sns.barplot(\n",
    "    x=content_df[content_df[\"http_code\"] != 200][\"http_code\"].value_counts().index,\n",
    "    y=content_df[content_df[\"http_code\"] != 200][\"http_code\"].value_counts().values / content_df.shape[0]*100,\n",
    "    ax=ax,\n",
    "    color=\"#31748f\",\n",
    ");\n",
    "\n",
    "# Setup the label\n",
    "ax.set_title(\"Distribution of HTTP Status Codes exluding 200\")\n",
    "ax.set_xlabel(\"HTTP Status Code\");\n",
    "ax.set_ylabel(\"Percentage of the total number of websites\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, look at the examples of redirects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get redirect webs\n",
    "redirect_webs = content_df[content_df[\"redirect_url\"].notna()]\n",
    "\n",
    "# Randomly sample 5 websites\n",
    "sampled_webs = redirect_webs.sample(5)\n",
    "\n",
    "for i, row in sampled_webs.iterrows():\n",
    "    print(f\"ℹ️ {row['original_url']} redirects to {row['redirect_url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that plus or minus the redirects are not an issue since most of the times the difference in the urls is just `/` the slash at the end of the url. Next, let's parse the html of the valid urls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get valid webs\n",
    "valid_webs = content_df[content_df[\"is_valid\"]].copy()\n",
    "\n",
    "# Save the features in a dict\n",
    "web_features = {}\n",
    "\n",
    "for i in tqdm(range(len(valid_webs))):\n",
    "    # Get html\n",
    "    html = valid_webs.iloc[i]['html']\n",
    "\n",
    "    # Get redirected url if available else original url\n",
    "    url = valid_webs.iloc[i]['redirect_url'] if valid_webs.iloc[i]['redirect_url'] else valid_webs.iloc[i]['original_url']\n",
    "\n",
    "    # Get id\n",
    "    wid = valid_webs.iloc[i]['webid']\n",
    "\n",
    "    # Get features\n",
    "    html_features = utils.parse_html(html, max_sentences=100)\n",
    "    url_features = utils.parse_url(url)\n",
    "    features = {**html_features, **url_features}\n",
    "\n",
    "    # Save the features\n",
    "    web_features[wid] = features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select random url and shows its extractred features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get random sample of 1 website\n",
    "sampled_web_id = valid_webs.sample(1)['webid'].values[0]\n",
    "\n",
    "# Get the features of the sampled website\n",
    "sampled_web_features = web_features[sampled_web_id]\n",
    "\n",
    "# Print the features\n",
    "for k, v in sampled_web_features.items():\n",
    "    print(f\"ℹ️ {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run compute some insights on the extracted features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of websites\n",
    "n = len(web_features)\n",
    "\n",
    "# Number of sites with title\n",
    "n_titles = len([title for title in web_features.values() if title['title'] is not None])\n",
    "print(f\"ℹ️ Number of sites with title: {n_titles/n*100:.2f}%\")\n",
    "\n",
    "# Number of sites with description\n",
    "n_descriptions = len([description for description in web_features.values() if description['description'] is not None])\n",
    "print(f\"ℹ️ Number of sites with description: {n_descriptions/n*100:.2f}%\")\n",
    "\n",
    "# Number of sites with keywords\n",
    "n_keywords = len([keywords for keywords in web_features.values() if keywords['keywords'] is not None])\n",
    "print(f\"ℹ️ Number of sites with keywords: {n_keywords/n*100:.2f}%\")\n",
    "\n",
    "# Number of sites with links\n",
    "n_links = len([links for links in web_features.values() if links['links'] is not None])\n",
    "print(f\"ℹ️ Number of sites with links: {n_links/n*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at the distribution of the number of meta tags and the number of links across the websites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of meta tags and links for each website\n",
    "meta_tags = []\n",
    "links = []\n",
    "for features in web_features.values():\n",
    "    if features['metatags'] is not None:\n",
    "        meta_tags.append(len(features['metatags']))\n",
    "    if features['links'] is not None:\n",
    "        links.append(len(features['links']))\n",
    "\n",
    "# Setup the figure\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot the distribution of meta tags\n",
    "sns.histplot(meta_tags, ax=axs[0], color=\"#31748f\")\n",
    "axs[0].set_title(\"Distribution of number of meta tags\")\n",
    "\n",
    "# Plot the distribution of links\n",
    "sns.histplot(links, ax=axs[1], color=\"#31748f\")\n",
    "axs[1].set_title(\"Distribution of number of links\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, let's compute the number of occurences of each meta tag and tld:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of occurrences of each metatag\n",
    "metatags = {}\n",
    "tlds = {}\n",
    "for features in web_features.values():\n",
    "    if features['metatags'] is not None:\n",
    "        for metatag in features['metatags']:\n",
    "            if metatag not in metatags:\n",
    "                metatags[metatag] = 1\n",
    "            else:\n",
    "                metatags[metatag] += 1\n",
    "    \n",
    "    if features['tld'] is not None:\n",
    "            tld = features['tld']\n",
    "            if tld not in tlds:\n",
    "                tlds[tld] = 1\n",
    "            else:\n",
    "                tlds[tld] += 1\n",
    "\n",
    "\n",
    "# Sort the metatags by number of occurrences\n",
    "metatags = {k: v for k, v in sorted(metatags.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "# Sort the tlds by number of occurrences\n",
    "tlds = {k: v for k, v in sorted(tlds.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "# Print the top 10 metatags\n",
    "print(\"ℹ️ Top 10 metatags:\")\n",
    "for i, (k, v) in enumerate(metatags.items()):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(f\"- {k}: {v/len(web_features)*100:.2f}%\")\n",
    "\n",
    "print()\n",
    "# Print the top 10 tlds\n",
    "print(\"ℹ️ Top 10 tlds:\")\n",
    "for i, (k, v) in enumerate(tlds.items()):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(f\"- {k}: {v/len(web_features)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's look into the average number of characters in the sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of characters in each sentence\n",
    "sentence_lengths = []\n",
    "for features in web_features.values():\n",
    "    if features['sentences'] is not None:\n",
    "        for sentence in features['sentences']:\n",
    "            sentence_lengths.append(len(sentence))\n",
    "\n",
    "# Setup the figure\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "# Plot the distribution of sentence lengths\n",
    "sns.histplot(sentence_lengths, ax=ax, color=\"#31748f\", log_scale=True);\n",
    "\n",
    "# Add labels\n",
    "ax.set_title(\"Distribution of sentence lengths\")\n",
    "ax.set_xlabel(\"Log Scale of Sentence length\")\n",
    "ax.set_ylabel(\"Number of sentences with the given length\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs502",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
