{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# EDA"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# ruff: noqa\n",
            "%reload_ext autoreload\n",
            "%autoreload 2\n",
            "\n",
            "# Standard library\n",
            "import os\n",
            "import json\n",
            "\n",
            "# External libraries\n",
            "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, cohen_kappa_score\n",
            "from matplotlib import pyplot as plt\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import seaborn as sns\n",
            "import rootutils\n",
            "import hydra\n",
            "\n",
            "# Local imports\n",
            "import ml_project_2_mlp.utils as utils"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Reinitialize hydra on every run\n",
            "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
            "h = hydra.initialize(config_path=\"../conf\", job_name=\"eda\", version_base=None)\n",
            "\n",
            "# Setup root environment\n",
            "root_path = rootutils.setup_root(\".\")\n",
            "rootutils.set_root(\n",
            "    path=root_path,\n",
            "    project_root_env_var=True,\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
<<<<<<< Updated upstream
         "metadata": {},
         "outputs": [],
         "source": [
            "# Global paths\n",
            "ROOT_DIR = root_path\n",
            "ARTIFACT_DIR = os.path.join(ROOT_DIR, \"artifacts\")\n",
            "FIGURE_DIR = os.path.join(ROOT_DIR, \"report\", \"figures\")\n",
            "TABLE_DIR = os.path.join(ROOT_DIR, \"report\", \"tables\")\n",
            "\n",
            "os.makedirs(FIGURE_DIR, exist_ok=True)\n",
            "os.makedirs(TABLE_DIR, exist_ok=True)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
=======
>>>>>>> Stashed changes
         "metadata": {},
         "outputs": [],
         "source": [
            "# Costs per token based on https://openai.com/pricing\n",
            "GPT4_COST_PER_INP_TOKEN = 0.00001\n",
            "GPT4_COST_PER_OUT_TOKEN = 0.00003\n",
            "GPT3_5_COST_PER_INP_TOKEN = 0.000001\n",
            "GPT3_5_COST_PER_OUT_TOKEN = 0.000002"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
<<<<<<< Updated upstream
=======
         "metadata": {},
         "outputs": [],
         "source": [
            "FIGURE_DIR = os.path.join(\"..\", \"report\", \"figures\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
>>>>>>> Stashed changes
         "metadata": {},
         "outputs": [],
         "source": [
            "rename_dict = {\n",
            "    \"human\": \"Human\",\n",
            "    \"gpt3.5\": \"GPT-3.5\",\n",
            "    \"gpt4\": \"GPT-4\",\n",
            "    \"context1\": \"C1\",\n",
            "    \"context2\": \"C2\",\n",
            "    \"context3\": \"C3\",\n",
            "    \"zeroshot\": \"0-shot\",\n",
            "    \"oneshot\": \"1-shot\",\n",
            "    \"f1\": \"Macro F1\",\n",
            "    \"acc\": \"Acc\",\n",
            "    \"precision\": \"Precision\",\n",
            "    \"recall\": \"Recall\",\n",
            "    \"lpp\": \"Labels Per Page\",\n",
            "    \"model\": \"Model\",\n",
            "    \"context\": \"Context\",\n",
            "    \"shot\": \"Shot\",\n",
            "    \"gpt-3.5-turbo-1106\": \"GPT-3.5\",\n",
            "    \"gpt-4-1106-preview\": \"GPT-4\",\n",
            "}\n",
            "\n",
            "def get_labeler_name(name: str):\n",
            "    return \" + \".join([rename_dict.get(n, n) for n in name.split(\"-\")])\n",
            "\n",
            "def get_metric_name(name: str):\n",
            "    if \"/\" in name:\n",
            "        split, metric = name.split(\"/\")\n",
            "        return f\"{rename_dict.get(split, split)} {rename_dict.get(metric, metric)}\"\n",
            "    else:\n",
            "        return rename_dict.get(name, name)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "style = \"dark\"\n",
            "palette = \"inferno\"\n",
            "sns.set_style(style)\n",
            "sns.set_palette(palette)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def get_num_features(feat, data):\n",
            "    return len([w[feat] for w in data.values() if w[feat] is not None and w[feat] != []])"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Websites\n",
            "\n",
            "---\n",
            "\n",
            "There are three copora of websites in this dataset:\n",
            "\n",
            "* `original`: 770 websites from the crowdsourced dataset from the [Homepage2Vec paper](https://arxiv.org/abs/1905.09786)\n",
            "* `gpt`: 250 common websites obtained by prompting GPT-4 (see [Prompts](https://chat.openai.com/share/a76c8b9b-a659-4b15-9ab0-d94af4733d58))\n",
            "* `curlie`: A filtered version of the [curlie](https://curlie.org) dataset, containing ~1M websites\n",
            "\n",
            "For each website, the repository contains a CSV file at the path `data/raw/<corpus>.csv` with the two columns - `wid` and `url`. The `wid` is a unique identifier for the website, and the `url` is the URL of the website."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Initialise Config\n",
            "original_cfg = hydra.compose(config_name=\"eda\", overrides=[\"data=original\"])\n",
            "gpt_cfg = hydra.compose(config_name=\"eda\", overrides=[\"data=gpt\"])\n",
            "curlie_cfg = hydra.compose(config_name=\"eda\", overrides=[\"data=curlie\"])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Load categories\n",
            "path = os.path.join(root_path, \"data\", \"meta\", \"categories.json\")\n",
            "with open(path) as f:\n",
            "    categories = json.load(f)\n",
            "\n",
            "print(f\"Classifying in {len(categories)} categories: {', '.join(categories)}\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "We will continue by exploring each of our datasets, we will store info about each of the indivudal datasets in a list of dictionaries."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "webinfo = []"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Original Data\n",
            "\n",
            "This is the data that was used to test the model in the original paper. "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Raw data\n",
            "original_data = hydra.utils.instantiate(original_cfg.data)\n",
            "\n",
            "raw_data = original_data.get_raw_data()\n",
            "processed_data = original_data.get_processed_data()\n",
            "embedded_data = original_data.get_embeddings()\n",
            "\n",
            "print(f\"Total number of samples: {len(raw_data)}\")\n",
            "raw_data.head(5)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Example of processed website\n",
            "wid = list(processed_data.keys())[0]\n",
            "data = processed_data[wid]\n",
            "\n",
            "print(f\"Collected data on {list(data.keys())}\")\n",
            "\n",
            "# Show some examples\n",
            "print(f\"\\nTitle: {data['title']}\")\n",
            "print(f\"Description: {data['description']}\")\n",
            "print(f\"Keywords: {data['keywords']}\")\n",
            "print(f\"Tags: {data['metatags']}\")\n",
            "print(f\"Domain: {data['domain']}\")\n",
            "print(f\"TLD: {data['tld']}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Setup info dict for original\n",
            "original_info = dict()\n",
            "\n",
            "# Save these into a dict\n",
            "original_info[\"n\"] = len(processed_data)\n",
            "original_info[\"tld\"] = get_num_features(\"tld\", processed_data) / original_info[\"n\"] * 100\n",
            "original_info[\"domain\"] = get_num_features(\"domain\", processed_data) / original_info[\"n\"] * 100\n",
            "original_info[\"tags\"] = get_num_features(\"metatags\", processed_data) / original_info[\"n\"] * 100\n",
            "original_info[\"titles\"] = get_num_features(\"title\", processed_data) / original_info[\"n\"] * 100\n",
            "original_info[\"descriptions\"] = get_num_features(\"description\", processed_data) / original_info[\"n\"] * 100\n",
            "original_info[\"keywords\"] = get_num_features(\"keywords\", processed_data) / original_info[\"n\"] * 100\n",
            "original_info[\"links\"] = get_num_features(\"links\", processed_data) / original_info[\"n\"] * 100\n",
            "original_info[\"sentences\"] = get_num_features(\"sentences\", processed_data) / original_info[\"n\"] * 100\n",
            "\n",
            "for k, v in original_info.items():\n",
            "    if k != \"n\":\n",
            "        print(f\"ℹ️ Percentage of sites with {k}: {v:.2f}%\")\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### GPT"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Raw data\n",
            "gpt_data = hydra.utils.instantiate(gpt_cfg.data)\n",
            "\n",
            "raw_data = gpt_data.get_raw_data()\n",
            "processed_data = gpt_data.get_processed_data()\n",
            "embedded_jdata = gpt_data.get_embeddings()\n",
            "\n",
            "print(f\"Total number of samples: {len(raw_data)}\")\n",
            "raw_data.head(5)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Example of processed website\n",
            "wid = list(processed_data.keys())[0]\n",
            "data = processed_data[wid]\n",
            "\n",
            "print(f\"Collected data on {list(data.keys())}\")\n",
            "\n",
            "# Show some examples\n",
            "print(f\"\\nTitle: {data['title']}\")\n",
            "print(f\"Description: {data['description']}\")\n",
            "print(f\"Keywords: {data['keywords']}\")\n",
            "print(f\"Tags: {data['metatags']}\")\n",
            "print(f\"Domain: {data['domain']}\")\n",
            "print(f\"TLD: {data['tld']}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "gpt_info = dict()\n",
            "\n",
            "# Save these into a dict\n",
            "gpt_info[\"n\"] = len(processed_data)\n",
            "gpt_info[\"tld\"] = get_num_features(\"tld\", processed_data) / gpt_info[\"n\"] * 100\n",
            "gpt_info[\"domain\"] = get_num_features(\"domain\", processed_data) / gpt_info[\"n\"] * 100\n",
            "gpt_info[\"tags\"] = get_num_features(\"metatags\", processed_data) / gpt_info[\"n\"] * 100\n",
            "gpt_info[\"titles\"] = get_num_features(\"title\", processed_data) /  gpt_info[\"n\"] * 100\n",
            "gpt_info[\"descriptions\"] = get_num_features(\"description\", processed_data) / gpt_info[\"n\"] * 100\n",
            "gpt_info[\"keywords\"] = get_num_features(\"keywords\", processed_data) / gpt_info[\"n\"] * 100\n",
            "gpt_info[\"links\"] = get_num_features(\"links\", processed_data) / gpt_info[\"n\"] * 100\n",
            "gpt_info[\"sentences\"] = get_num_features(\"sentences\", processed_data) / gpt_info[\"n\"] * 100\n",
            "\n",
            "for k, v in gpt_info.items():\n",
            "    if k != \"n\":\n",
            "        print(f\"ℹ️ Percentage of sites with {k}: {v:.2f}%\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Curlie"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Raw data\n",
            "curlie_data = hydra.utils.instantiate(curlie_cfg.data)\n",
            "\n",
            "raw_data = curlie_data.get_raw_data()\n",
            "processed_data = curlie_data.get_processed_data()\n",
            "embedded_jdata = curlie_data.get_embeddings()\n",
            "\n",
            "print(f\"Total number of samples: {len(raw_data)}\")\n",
            "raw_data.head(5)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Example of processed website\n",
            "wid = list(processed_data.keys())[0]\n",
            "data = processed_data[wid]\n",
            "\n",
            "print(f\"Collected data on {list(data.keys())}\")\n",
            "\n",
            "# Show some examples\n",
            "print(f\"\\nTitle: {data['title']}\")\n",
            "print(f\"Description: {data['description']}\")\n",
            "print(f\"Keywords: {data['keywords']}\")\n",
            "print(f\"Tags: {data['metatags']}\")\n",
            "print(f\"Domain: {data['domain']}\")\n",
            "print(f\"TLD: {data['tld']}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "curlie_info = dict()\n",
            "\n",
            "# Save these into a dict\n",
            "curlie_info[\"n\"] = len(processed_data)\n",
            "curlie_info[\"tld\"] = get_num_features(\"tld\", processed_data) / curlie_info[\"n\"] * 100\n",
            "curlie_info[\"domain\"] = get_num_features(\"domain\", processed_data) / curlie_info[\"n\"] * 100\n",
            "curlie_info[\"tags\"] = get_num_features(\"metatags\", processed_data) / curlie_info[\"n\"] * 100\n",
            "curlie_info[\"titles\"] = get_num_features(\"title\", processed_data) / curlie_info[\"n\"] * 100\n",
            "curlie_info[\"descriptions\"] = get_num_features(\"description\", processed_data) / curlie_info[\"n\"] * 100\n",
            "curlie_info[\"keywords\"] = get_num_features(\"keywords\", processed_data) / curlie_info[\"n\"] * 100\n",
            "curlie_info[\"links\"] = get_num_features(\"links\", processed_data) / curlie_info[\"n\"] * 100\n",
            "curlie_info[\"sentences\"] = get_num_features(\"sentences\", processed_data) / curlie_info[\"n\"] * 100\n",
            "\n",
            "for k, v in curlie_info.items():\n",
            "    if k != \"n\":\n",
            "        print(f\"ℹ️ Percentage of sites with {k}: {v:.2f}%\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### LaTeX Table"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Put the data info into a dataframe\n",
            "df = pd.DataFrame([original_info, curlie_info], index=[\"Original\", \"Curlie-gpt-10k\"]).round(2)\n",
            "\n",
            "# Change all columns names to include (%)\n",
            "df.columns = [f\"{c} (%)\" if c != \"n\" else c for c in df.columns]\n",
            "\n",
            "df = df.T\n",
            "\n",
            "# Save the dataframe to a latex table\n",
            "position = \"!ht\"\n",
            "save_path = os.path.join(root_path, \"report\", \"tables\", \"feature_information.tex\")\n",
            "latex = df.to_latex(\n",
            "    caption=\"Percentage of websites with each feature accross our datasets.\", \n",
            "    label=\"tab:feature_information\",\n",
            "    escape=True,\n",
            "    float_format=\"%.2f\",\n",
            "    position=position\n",
            ")\n",
            "\n",
            "# Add \\centering right after \\begin{table}\n",
            "latex = latex.replace(\"\\\\begin{table}\" + f\"[{position}]\", \"\\\\begin{table}\" + f\"[{position}]\" + \"\\n\\\\centering\")\n",
            "with open(save_path, \"w\") as f:\n",
            "    f.write(latex)\n",
            "\n",
            "df.head()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Labelers\n",
            "\n",
            "---\n",
            "\n",
            "There are multiple GPT labeler instances that can be used to label the data. The labelers are defined in the labelers module and are identified by a `context`, `model` and `fewshow` parameter. The `context` parameter defines the context that is used to prompt the model. The `model` parameter defines the model that is used to generate the labels. The `fewshot` parameter defines whether the model is trained in a few-shot manner or not.\n",
            "\n",
            "| Parameter | Variants | Description |\n",
            "| --- | --- | ---  |\n",
            "| `context` | `context1` | Uses the `tld`, `domain` and `metatags` as context |\n",
            "| | `context2` | Uses the `tld`, `domain`, `metatags`, `links` and `text` as context |\n",
            "| | `context3` | Uses the `tld`, `domain`, `metatags`, `links`, `text` and `images` as context |\n",
            "| `model` | `gpt3.5` | Uses GPT-3.5 (`gpt-3.5-turbo-1106`) |\n",
            "| | `gpt4` | Uses GPT-4 (`gpt-4-1106-preview`) |\n",
            "| `fewshot` | `fewshot` | Injects an example website and label into the system prompt |\n",
            "| | `zeroshot` | Does not inject any example website or label into the system prompt |\n",
            "\n",
            "We are considering all contexts and few-shot settings for the GPT-3.5 model, but only the `context2` for the GPT-4 model. Additionally, we have the `human` labeler which simply loads the annotatations for the `original` dataset from the [Homepage2Vec paper](https://arxiv.org/abs/1905.09786)."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Initialise configuration for all labelers\n",
            "gpt_labeler_names = [\n",
            "    \"gpt3.5-zeroshot-context1\", \n",
            "    \"gpt3.5-oneshot-context1\",\n",
            "    \"gpt3.5-zeroshot-context2\",\n",
            "    \"gpt3.5-oneshot-context2\",\n",
            "    \"gpt3.5-zeroshot-context3\",\n",
            "    \"gpt3.5-oneshot-context3\",\n",
            "    \"gpt4-zeroshot-context1\",\n",
            "    \"gpt4-oneshot-context1\",\n",
            "    \"gpt4-zeroshot-context2\",\n",
            "    \"gpt4-oneshot-context2\",\n",
            "    # \"gpt4-zeroshot-context3\",\n",
            "    \"gpt4-oneshot-context3\"\n",
            "] \n",
            "\n",
            "gpt_labelers_cfg = {labeler: hydra.compose(config_name=\"eda\", overrides=[f\"labeler={labeler}\"]) for labeler in gpt_labeler_names}"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Instantiate data\n",
            "original_data_cfg = hydra.compose(config_name=\"eda\", overrides=[\"data=original\"])\n",
            "original_data = hydra.utils.instantiate(original_data_cfg.data)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Instantiate labelers\n",
            "gpt_labelers = {labeler: hydra.utils.instantiate(cfg.labeler, data=original_data) for labeler, cfg in gpt_labelers_cfg.items()}"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Instatntiate labelers dataframe\n",
            "def get_info(labeler):\n",
            "    return {\n",
            "        \"model\": labeler.model,\n",
            "        \"context\": labeler.name.split(\"-\")[-1],\n",
            "        \"shot\": labeler.fewshot,\n",
            "    }\n",
            "\n",
            "labeler_info = pd.DataFrame(\n",
            "    [get_info(labeler) for labeler in gpt_labelers.values()],\n",
            "    index=[get_labeler_name(name) for name in gpt_labelers.keys()])\n",
            "labeler_info"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Let's verify that the labelers are working as expected by checking the number of labeled webpages."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "num_processed_websites = len(original_data.get_processed_data())\n",
            "\n",
            "print(f\"ℹ️ Number of processed websites: {num_processed_websites}\")\n",
            "for name, labeler in gpt_labelers.items():\n",
            "    num_labels = len(labeler.get_labels())\n",
            "    print(f\"ℹ️ Number of {name} labels: {num_labels}\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Labeling statistics\n",
            "\n",
            "Let's investigate some statistics about the labelers. We will compute:\n",
            "\n",
            "* The average number of labels per website\n",
            "* The number of valid labels\n",
            "* The number of invalid labels\n",
            "* The average time taken to label a website\n",
            "* The average number of prompt and completion tokens used to label a website\n",
            "* The estimated cost of labeling the entire dataset"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import stat\n",
            "\n",
            "\n",
            "def get_statistics(labeler):\n",
            "    statistics = {\"valid\": 0, \"invalid\": 0, \"lpp\": [], \"durations\": [], \"prompt_tokens\": [], \"completion_tokens\": []}\n",
            "    for website in labeler.get_labels().values():\n",
            "        if not website[\"is_valid\"]:\n",
            "            statistics[\"invalid\"] += 1\n",
            "            continue\n",
            "\n",
            "        statistics[\"valid\"] += 1\n",
            "        statistics[\"lpp\"].append(sum(website[\"labels\"]))\n",
            "        statistics[\"durations\"].append(website[\"duration\"])\n",
            "        statistics[\"prompt_tokens\"].append(website[\"prompt_tokens\"])\n",
            "        statistics[\"completion_tokens\"].append(website[\"completion_tokens\"])\n",
            "\n",
            "    lpps = np.array(statistics[\"lpp\"])\n",
            "    durations = np.array(statistics[\"durations\"])\n",
            "    prompt_tokens = np.array(statistics[\"prompt_tokens\"])\n",
            "    completion_tokens = np.array(statistics[\"completion_tokens\"])\n",
            "\n",
            "    statistics[\"lpp\"] = f\"{lpps.mean():.2f} ± {lpps.std():.2f}\"\n",
            "    statistics[\"durations\"] = f\"{durations.mean():.2f} ± {durations.std():.2f}\"\n",
            "    statistics[\"prompt_tokens\"] = f\"{prompt_tokens.mean():.2f} ± {prompt_tokens.std():.2f}\"\n",
            "    statistics[\"completion_tokens\"] = f\"{completion_tokens.mean():.2f} ± {completion_tokens.std():.2f}\"\n",
            "    \n",
            "    # Compute estimated cost\n",
            "    model = labeler.model\n",
            "    COST_PER_INP_TOKEN = GPT4_COST_PER_INP_TOKEN if \"gpt-4\" in model else GPT3_5_COST_PER_INP_TOKEN\n",
            "    COST_PER_OUT_TOKEN = GPT4_COST_PER_OUT_TOKEN if \"gpt-4\" in model else GPT3_5_COST_PER_OUT_TOKEN\n",
            "\n",
            "    statistics[\"estimated_cost\"] = (prompt_tokens.sum() * COST_PER_INP_TOKEN + completion_tokens.sum() * COST_PER_OUT_TOKEN)\n",
            "    statistics[\"cost_per_1k_page\"] = 1000 * statistics[\"estimated_cost\"] / statistics[\"valid\"]\n",
            "\n",
            "    return statistics"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "labeler_statistics = pd.DataFrame(\n",
            "    [get_statistics(labeler) for labeler in gpt_labelers.values()], \n",
            "    index=[get_labeler_name(name) for name in gpt_labelers.keys()]) \n",
            "labeler_statistics"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Class Distribution"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def get_label_dist(labelers):\n",
            "    rows = []\n",
            "    for name, class_dist in {name: labeler.get_class_dist(normalise=True) for name, labeler in labelers.items()}.items():\n",
            "        for category, freq in class_dist.items():\n",
            "            rows.append({\"labeler\": get_labeler_name(name), \"category\": category, \"freq\": 100 * freq})\n",
            "    return pd.DataFrame(rows)\n",
            "\n",
            "label_dist = get_label_dist(gpt_labelers)\n",
            "fig, ax = plt.subplots(figsize=(15, 6))\n",
            "fig.tight_layout(pad=3.0)\n",
            "sns.barplot(\n",
            "    label_dist,\n",
            "    x=\"category\",\n",
            "    y=\"freq\",\n",
            "    hue=\"labeler\",\n",
            "    palette=palette,\n",
            "    order=label_dist.groupby(\"category\").sum().sort_values(\"freq\", ascending=False).index,\n",
            "    ax=ax\n",
            ")\n",
            "ax.get_legend().set_title(\"Labeler\")\n",
            "ax.set_xlabel(\"Category\", fontsize=14)\n",
            "ax.set_ylabel(\"Frequency (%)\", fontsize=14)\n",
            "ax.set_title(\"Original Label Distribution\", fontsize=14)\n",
            "\n",
            "path = os.path.join(FIGURE_DIR, \"original-label-distribution.pdf\")\n",
            "fig.savefig(path, bbox_inches=\"tight\")\n",
            "print(f\"✅ Saved figure to {path}\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Labeling Quality: Macro F1, Precision, Recall, etc.\n",
            "\n",
            "The goal of all GPT labelers is to replicate the ground truth labels provide by the human annotators as closely as possible. As we only have human annotations for the original dataset, we can only evaluate the labelers on this dataset."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Instatiate human labeler\n",
            "original_cfg = hydra.compose(config_name=\"eda\", overrides=[\"data=original\"])\n",
            "human_labeler_cfg = hydra.compose(config_name=\"eda\", overrides=[\"labeler=human\", \"data=original\"])\n",
            "original_data = hydra.utils.instantiate(original_cfg.data)\n",
            "human_labeler = hydra.utils.instantiate(human_labeler_cfg.labeler, data=original_data)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def match_labels(labeler1, labeler2, subset = None):\n",
            "    labels1 = labeler1.get_labels()\n",
            "    labels2 = labeler2.get_labels()\n",
            "    wid1 = set(labels1.keys())\n",
            "    wid2 = set(labels2.keys())\n",
            "    matched_wid = wid1 & wid2\n",
            "    if subset:\n",
            "        matched_wid = matched_wid & subset\n",
            "\n",
            "    labels1 = np.array([labels1[wid][\"labels\"] for wid in matched_wid])\n",
            "    labels2 = np.array([labels2[wid][\"labels\"] for wid in matched_wid])\n",
            "\n",
            "    return labels1, labels2"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def get_labeler_perf(labeler):\n",
            "    labels1, labels2 = match_labels(human_labeler, labeler)\n",
            "\n",
            "    acc = accuracy_score(labels1.flatten(), labels2.flatten())\n",
            "    subset_acc = accuracy_score(labels1, labels2)\n",
            "    macro_f1 = f1_score(labels1, labels2, average=\"macro\")\n",
            "    micro_f1 = f1_score(labels1, labels2, average=\"micro\")\n",
            "    weighted_f1 = f1_score(labels1, labels2, average=\"weighted\")\n",
            "    macro_precision = precision_score(labels1, labels2, average=\"macro\", zero_division=0)\n",
            "    micro_precision = precision_score(labels1, labels2, average=\"micro\", zero_division=0)\n",
            "    macro_recall = recall_score(labels1, labels2, average=\"macro\", zero_division=0)\n",
            "    micro_recall = recall_score(labels1, labels2, average=\"micro\", zero_division=0)\n",
            "\n",
            "    return {\n",
            "        \"acc\": acc,\n",
            "        \"subset_acc\": subset_acc,\n",
            "        \"macro_f1\": macro_f1,\n",
            "        \"micro_f1\": micro_f1,\n",
            "        \"weighted_f1\": weighted_f1,\n",
            "        \"macro_precision\": macro_precision,\n",
            "        \"micro_precision\": micro_precision,\n",
            "        \"macro_recall\": macro_recall,\n",
            "        \"micro_recall\": micro_recall,\n",
            "    }"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "labeler_perf = pd.DataFrame([get_labeler_perf(labeler) for labeler in gpt_labelers.values()], \n",
            "                            index=[get_labeler_name(name) for name in gpt_labelers.keys()])\n",
            "labeler_perf"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Analysis"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Join with labeling statistics\n",
            "labelers_df = pd.concat([labeler_info, labeler_statistics, labeler_perf], axis=1)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Top-k performing labeler\n",
            "top_k = 5\n",
            "labelers_df.sort_values(\"macro_f1\", ascending=False).head(top_k)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
<<<<<<< Updated upstream
         "source": [
            "# Export table to latex\n",
            "df = labelers_df.copy()\n",
            "\n",
            "rename_dict = {\"gpt-3.5-turbo-1106\": \"GPT-3.5\", \"gpt-4-1106-preview\": \"GPT-4\", \"context1\": \"C1\", \"context2\": \"C2\", \"context3\": \"C3\", False: \"0-shot\", True: \"1-shot\"}\n",
            "\n",
            "df[\"model\"] = df[\"model\"].apply(lambda x: rename_dict[x])\n",
            "df[\"context\"] = df[\"context\"].apply(lambda x: rename_dict[x])\n",
            "df[\"shot\"] = df[\"shot\"].apply(lambda x: rename_dict[x])\n",
            "df[\"macro_f1\"] = 100 * df[\"macro_f1\"]\n",
            "\n",
            "grouped = df.groupby(by=[\"model\", \"context\", \"shot\"]).size()\n",
            "df.index = pd.MultiIndex.from_tuples(grouped.index)\n",
            "\n",
            "cols = {\"lpp\": \"LPP\", \"cost_per_1k_page\": \"Cost ($)\", \"macro_f1\": \"M.-F1 (%)\"}\n",
            "df = df[cols.keys()].rename(columns=cols)\n",
            "\n",
            "# Save the dataframe to a latex table\n",
            "position = \"!ht\"\n",
            "save_path = os.path.join(TABLE_DIR, \"labeler-results.tex\")\n",
            "latex = df.to_latex(\n",
            "    caption=\"\\\\textbf{Labeler Statistics.}\", \n",
            "    label=\"tab:labeler-results\",\n",
            "    escape=True,\n",
            "    float_format=\"%.2f\",\n",
            "    position=position,\n",
            "    multirow=True,\n",
            "    multicolumn=True,\n",
            "    multicolumn_format=\"c\",\n",
            ")\n",
            "\n",
            "# Add \\centering right after \\begin{table}\n",
            "latex = latex.replace(\"\\\\begin{table}\" + f\"[{position}]\", \"\\\\begin{table}\" + f\"[{position}]\" + \"\\n\\\\centering\")\n",
            "latex = latex.replace(\"[t]\", \"[c]\")\n",
            "with open(save_path, \"w\") as f:\n",
            "    f.write(latex)\n",
            "print(f\"✅ Saved table to {save_path}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Time-cost tradeoff\n",
            "df = labelers_df.copy()\n",
            "df[\"time\"] = df[\"durations\"].apply(lambda x: x.split(\" ± \")[0]).astype(float)\n",
            "df[\"cost\"] = df[\"estimated_cost\"].astype(float)\n",
            "\n",
            "fig, ax = plt.subplots(figsize=(5, 5))\n",
            "sns.scatterplot(\n",
            "    data=df,\n",
            "    x=\"cost\",\n",
            "    y=\"macro_f1\",\n",
            "    style=\"model\",\n",
            "    hue=\"context\",\n",
            "    palette=palette,\n",
            "    s=200,\n",
            "    alpha=0.8,\n",
            ")\n",
            "ax.set_ylim(0, 0.5)\n",
            "ax.set_xlabel(\"Cost ($)\")\n",
            "ax.set_ylabel(\"Label Quality (M.-F1)\")\n",
            "\n",
            "# Rename legend labels\n",
            "handles, labels = ax.get_legend_handles_labels()\n",
            "labels = [f\"{rename_dict.get(label, label)}\" for label in labels]\n",
            "ax.legend(handles=handles, labels=labels)\n",
            "\n",
            "path = os.path.join(FIGURE_DIR, \"time-cost-tradeoff.pdf\")\n",
            "fig.savefig(path, bbox_inches=\"tight\")\n",
            "print(f\"✅ Saved figure to {path}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
=======
>>>>>>> Stashed changes
         "source": [
            "# Univariate performance\n",
            "fig, axs = plt.subplots(ncols=3, figsize=(20, 5))\n",
            "xs = [\"model\", \"context\", \"shot\"]\n",
            "\n",
            "for ax, x in zip(axs, xs):\n",
            "    sns.barplot(\n",
            "        data=labelers_df,\n",
            "        x=x,\n",
            "        y=\"macro_f1\",\n",
            "        ax=ax\n",
            "    )\n",
            "    ax.set_xticks(ax.get_xticks())\n",
            "    ax.set_xticklabels([rename_dict.get(t.get_text(), t.get_text()) for t in ax.get_xticklabels()])\n",
            "    ax.set_xlabel(x.capitalize(), fontsize=14)\n",
            "    ax.set_ylabel(\"\")\n",
            "\n",
            "axs[0].set_ylabel(\"Macro F1\", fontsize=14)\n",
            "\n",
            "print(f\"✅ Plotted labeler quality as function of model, context and shot\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Labeler parameters\n",
            "params = [\"model\", \"context\", \"shot\"]\n",
            "\n",
            "fig = utils.grid(labelers_df, params, metric=\"mean\", cmap=\"Greens\", figsize=(8, 8), rename_dict=rename_dict) \n",
            "\n",
            "path = os.path.join(FIGURE_DIR, \"labelers-grid.pdf\")\n",
            "fig.savefig(path, bbox_inches=\"tight\")\n",
            "print(f\"✅ Saved figure to {path}\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Labeling Quality: Annotator agreeement between GPT and Human\n",
            "\n",
            "We have seen in the [preprocesssing notebook](preprocessing.ipynb) that the inter annotator agreement per website on average between humans measured via Cohen's Kappa was already relatively low ($0.2$). In this section, we therefore focus on measuring the agreement level between the GPT labelers and the human annotators."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "gpt_human_agg = dict()\n",
            "human_labels = np.array([v[\"labels\"] for v in human_labeler.get_labels().values()])\n",
            "for name, labeler in gpt_labelers.items():\n",
            "\n",
            "    # Obtain 2d array with labels for each website (n_websites, n_categories)\n",
            "    gpt_labels = np.array([v[\"labels\"] for v in labeler.get_labels().values()])\n",
            "    k = gpt_labels.shape[1]\n",
            "\n",
            "    # Ensure matching between human and gpt labels\n",
            "    human_labs_matched, gpt_labs_matched = match_labels(human_labeler, labeler)\n",
            "\n",
            "    kappa_scores = []\n",
            "    for j in range(k):\n",
            "\n",
            "        # Obtain the labels for kth class by human and gpt\n",
            "        human_kthcls_labels = human_labs_matched[:, j]\n",
            "        gpt_kthcls_labels = gpt_labs_matched[:, j]\n",
            "\n",
            "        # Compute the cohens kappa score\n",
            "        kappa = cohen_kappa_score(human_kthcls_labels, gpt_kthcls_labels)\n",
            "\n",
            "        # Append to list\n",
            "        kappa_scores.append(kappa)\n",
            "\n",
            "\n",
            "    # Save the results\n",
            "    gpt_human_agg[name] = kappa_scores"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Let's now evalute the results, first we look at the overall picture using dataframe:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "aggreement_df = pd.DataFrame(gpt_human_agg, index=categories).round(2)\n",
            "aggreement_df"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Let's now get some insights about the statistics of the agreement level between the GPT labelers and the human annotators:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "agreement_stats = aggreement_df.describe().round(2)\n",
            "agreement_stats"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "We can see that as the complexity of the labeler increases, the more average agreement we obtain with the human annotators with plus or minus same standard deviation. Interestingly, we can see that for some categories  and gpt4 based labelers we obtain almost $0.5$ agreement. "
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Curlie\n",
            "\n",
            "In this section we explore the labels of the `curlie-1000` that we relabel with the most promising GPT labelers given the label quality and cost of labeling as shown in the section above."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Initialise data and labeler config\n",
            "curlie_cfg = hydra.compose(config_name=\"eda\", overrides=[\"data=curlie\"])\n",
            "curlie_labeler_names = [\"gpt3.5-oneshot-context2\"]\n",
            "curlie_labeler_cfg = {name: hydra.compose(config_name=\"eda\", overrides=[f\"labeler={name}\"]) for name in curlie_labeler_names}"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Instantiate data\n",
            "curlie_data = hydra.utils.instantiate(curlie_cfg.data)\n",
            "curlie_labeler = {name: hydra.utils.instantiate(curlie_labeler.labeler, data=curlie_data) for name, curlie_labeler in curlie_labeler_cfg.items()}\n",
            "print(f\"✅ Initialised {len(curlie_labeler)} labeler(s).\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "We expect that the LPP will be higher as the labelers have learned to replicate the human labels."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "values = [get_statistics(labeler) for labeler in curlie_labeler.values()]\n",
            "index = [get_labeler_name(name) for name in curlie_labeler.keys()]\n",
            "labeler_statistics = pd.DataFrame(values, index=index)\n",
            "labeler_statistics"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Let's first look at the distribution of labels for each category."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Plot label distribution\n",
            "label_dist = get_label_dist(curlie_labeler)\n",
            "fig, ax = plt.subplots(figsize=(4, 5))\n",
            "fig.tight_layout(pad=3.0)\n",
            "sns.barplot(\n",
            "    label_dist,\n",
            "    x=\"freq\",\n",
            "    y=\"category\",\n",
            "    hue=\"labeler\",\n",
            "    palette=\"inferno\",\n",
            "    order=label_dist.groupby(\"category\").sum().sort_values(\"freq\", ascending=False).index,\n",
            "    ax=ax\n",
            ")\n",
            "ax.get_legend().set_title(\"Labeler\")\n",
            "ax.set_xlabel(\"Percentage\")\n",
            "ax.set_ylabel(\"\")\n",
            "\n",
            "# To each bar assign the corresponding percentage\n",
            "for i, v in enumerate(label_dist.groupby(\"category\").sum().sort_values(\"freq\", ascending=False)[\"freq\"]):\n",
            "    ax.text(v + 1, i, f\"{v:.1f}\", color=\"black\", va=\"center\", fontsize=11)\n",
            "\n",
            "ax.set_xlim(0, 50)\n",
            "\n",
            "path = os.path.join(FIGURE_DIR, \"curlie-label-distribution.pdf\")\n",
            "fig.savefig(path, bbox_inches=\"tight\")\n",
            "print(f\"✅ Saved figure to {path}\")"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "ml-project-2-mlp-a6NSXBdT-py3.10",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.13"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
