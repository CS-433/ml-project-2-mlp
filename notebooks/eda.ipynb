{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ludekcizinsky/Library/Caches/pypoetry/virtualenvs/ml-project-2-mlp-rx2AOdW0-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ruff: noqa\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Standard library\n",
    "import os\n",
    "import json\n",
    "\n",
    "# External libraries\n",
    "from sklearn.metrics import classification_report\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import rootutils\n",
    "import hydra\n",
    "\n",
    "# Local imports\n",
    "import ml_project_2_mlp.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize hydra on every run\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "h = hydra.initialize(config_path=\"../conf\", job_name=\"eda\", version_base=None)\n",
    "\n",
    "# Setup root environment\n",
    "root_path = rootutils.setup_root(\".\")\n",
    "rootutils.set_root(\n",
    "    path=root_path,\n",
    "    project_root_env_var=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Costs per token based on https://openai.com/pricing\n",
    "GPT4_COST_PER_INP_TOKEN = 0.00001\n",
    "GPT4_COST_PER_OUT_TOKEN = 0.00003\n",
    "GPT3_5_COST_PER_INP_TOKEN = 0.000001\n",
    "GPT3_5_COST_PER_OUT_TOKEN = 0.000002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"dark\")\n",
    "sns.set_palette(\"gist_stern\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_featues(feat, data):\n",
    "    return len([w[feat] for w in data.values() if w[feat] is not None and w[feat] != []])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Websites\n",
    "\n",
    "---\n",
    "\n",
    "There are three copora of websites in this dataset:\n",
    "\n",
    "* `original`: 770 websites from the crowdsourced dataset from the [Homepage2Vec paper](https://arxiv.org/abs/1905.09786)\n",
    "* `gpt`: 250 common websites obtained by prompting GPT-4 (see [Prompts](https://chat.openai.com/share/a76c8b9b-a659-4b15-9ab0-d94af4733d58))\n",
    "* `curlie`: A filtered version of the [curlie](https://curlie.org) dataset, containing ~1M websites\n",
    "\n",
    "For each website, the repository contains a CSV file at the path `data/raw/<corpus>.csv` with the two columns - `wid` and `url`. The `wid` is a unique identifier for the website, and the `url` is the URL of the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Config\n",
    "original_cfg = hydra.compose(config_name=\"eda\", overrides=[\"data=original\"])\n",
    "gpt_cfg = hydra.compose(config_name=\"eda\", overrides=[\"data=gpt\"])\n",
    "curlie_cfg = hydra.compose(config_name=\"eda\", overrides=[\"data=curlie\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying in 14 categories: Arts, Business, Computers, Games, Health, Home, Kids_and_Teens, News, Recreation, Reference, Science, Shopping, Society, Sports\n"
     ]
    }
   ],
   "source": [
    "# Load categories\n",
    "path = os.path.join(root_path, \"data\", \"meta\", \"categories.json\")\n",
    "with open(path) as f:\n",
    "    categories = json.load(f)\n",
    "\n",
    "print(f\"Classifying in {len(categories)} categories: {', '.join(categories)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continue by exploring each of our datasets, we will store info about each of the indivudal datasets in a list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "webinfo = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Data\n",
    "\n",
    "This is the data that was used to test the model in the original paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 840\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wid</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1161124</td>\n",
       "      <td>www.pointlesssites.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1081241</td>\n",
       "      <td>www.connecticutplastics.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1162420</td>\n",
       "      <td>99percentinvisible.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1146040</td>\n",
       "      <td>www.medicaid.gov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1117243</td>\n",
       "      <td>www.graalonline.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       wid                          url\n",
       "0  1161124       www.pointlesssites.com\n",
       "1  1081241  www.connecticutplastics.com\n",
       "2  1162420       99percentinvisible.org\n",
       "3  1146040             www.medicaid.gov\n",
       "4  1117243          www.graalonline.com"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Raw data\n",
    "original_data = hydra.utils.instantiate(original_cfg.data)\n",
    "\n",
    "raw_data = original_data.get_raw_data()\n",
    "processed_data = original_data.get_processed_data()\n",
    "embedded_data = original_data.get_embeddings()\n",
    "\n",
    "print(f\"Total number of samples: {len(raw_data)}\")\n",
    "raw_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected data on ['title', 'description', 'keywords', 'links', 'sentences', 'metatags', 'tld', 'domain']\n",
      "\n",
      "Title: PointlessSites.com  Fun Things To Do When You're Bored\n",
      "Description: Are you bored? Want something fun to do? Check out these funny websites, pointless facts and stupid pictures brought to you by Pointless Sites!\n",
      "Keywords: ['Pointless', 'Sites', 'portal,', 'useless', 'pointlessness']\n",
      "Tags: ['description', 'viewport', 'author', 'keywords', 'robots', 'copyright']\n",
      "Domain: pointlesssites\n",
      "TLD: com\n"
     ]
    }
   ],
   "source": [
    "# Example of processed website\n",
    "wid = list(processed_data.keys())[0]\n",
    "data = processed_data[wid]\n",
    "\n",
    "print(f\"Collected data on {list(data.keys())}\")\n",
    "\n",
    "# Show some examples\n",
    "print(f\"\\nTitle: {data['title']}\")\n",
    "print(f\"Description: {data['description']}\")\n",
    "print(f\"Keywords: {data['keywords']}\")\n",
    "print(f\"Tags: {data['metatags']}\")\n",
    "print(f\"Domain: {data['domain']}\")\n",
    "print(f\"TLD: {data['tld']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ Percentage of sites with tld: 100.00%\n",
      "ℹ️ Percentage of sites with domain: 100.00%\n",
      "ℹ️ Percentage of sites with tags: 93.69%\n",
      "ℹ️ Percentage of sites with titles: 98.42%\n",
      "ℹ️ Percentage of sites with descriptions: 54.93%\n",
      "ℹ️ Percentage of sites with keywords: 19.58%\n",
      "ℹ️ Percentage of sites with links: 89.88%\n",
      "ℹ️ Percentage of sites with sentences: 99.08%\n"
     ]
    }
   ],
   "source": [
    "# Setup info dict for original\n",
    "original_info = dict()\n",
    "\n",
    "# Save these into a dict\n",
    "original_info[\"n\"] = len(processed_data)\n",
    "original_info[\"tld\"] = get_num_featues(\"tld\", processed_data) / original_info[\"n\"] * 100\n",
    "original_info[\"domain\"] = get_num_featues(\"domain\", processed_data) / original_info[\"n\"] * 100\n",
    "original_info[\"tags\"] = get_num_featues(\"metatags\", processed_data) / original_info[\"n\"] * 100\n",
    "original_info[\"titles\"] = get_num_featues(\"title\", processed_data) / original_info[\"n\"] * 100\n",
    "original_info[\"descriptions\"] = get_num_featues(\"description\", processed_data) / original_info[\"n\"] * 100\n",
    "original_info[\"keywords\"] = get_num_featues(\"keywords\", processed_data) / original_info[\"n\"] * 100\n",
    "original_info[\"links\"] = get_num_featues(\"links\", processed_data) / original_info[\"n\"] * 100\n",
    "original_info[\"sentences\"] = get_num_featues(\"sentences\", processed_data) / original_info[\"n\"] * 100\n",
    "\n",
    "for k, v in original_info.items():\n",
    "    if k != \"n\":\n",
    "        print(f\"ℹ️ Percentage of sites with {k}: {v:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 250\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wid</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>google.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>facebook.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>youtube.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>amazon.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>wikipedia.org</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   wid            url\n",
       "0    0     google.com\n",
       "1    1   facebook.com\n",
       "2    2    youtube.com\n",
       "3    3     amazon.com\n",
       "4    4  wikipedia.org"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Raw data\n",
    "gpt_data = hydra.utils.instantiate(gpt_cfg.data)\n",
    "\n",
    "raw_data = gpt_data.get_raw_data()\n",
    "processed_data = gpt_data.get_processed_data()\n",
    "embedded_jdata = gpt_data.get_embeddings()\n",
    "\n",
    "print(f\"Total number of samples: {len(raw_data)}\")\n",
    "raw_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected data on ['title', 'description', 'keywords', 'links', 'sentences', 'metatags', 'tld', 'domain']\n",
      "\n",
      "Title: Google\n",
      "Description: None\n",
      "Keywords: []\n",
      "Tags: ['referrer']\n",
      "Domain: google\n",
      "TLD: com\n"
     ]
    }
   ],
   "source": [
    "# Example of processed website\n",
    "wid = list(processed_data.keys())[0]\n",
    "data = processed_data[wid]\n",
    "\n",
    "print(f\"Collected data on {list(data.keys())}\")\n",
    "\n",
    "# Show some examples\n",
    "print(f\"\\nTitle: {data['title']}\")\n",
    "print(f\"Description: {data['description']}\")\n",
    "print(f\"Keywords: {data['keywords']}\")\n",
    "print(f\"Tags: {data['metatags']}\")\n",
    "print(f\"Domain: {data['domain']}\")\n",
    "print(f\"TLD: {data['tld']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ Percentage of sites with tld: 100.00%\n",
      "ℹ️ Percentage of sites with domain: 100.00%\n",
      "ℹ️ Percentage of sites with tags: 98.17%\n",
      "ℹ️ Percentage of sites with titles: 96.35%\n",
      "ℹ️ Percentage of sites with descriptions: 86.30%\n",
      "ℹ️ Percentage of sites with keywords: 22.37%\n",
      "ℹ️ Percentage of sites with links: 85.84%\n",
      "ℹ️ Percentage of sites with sentences: 96.35%\n"
     ]
    }
   ],
   "source": [
    "gpt_info = dict()\n",
    "\n",
    "# Save these into a dict\n",
    "gpt_info[\"n\"] = len(processed_data)\n",
    "gpt_info[\"tld\"] = get_num_featues(\"tld\", processed_data) / gpt_info[\"n\"] * 100\n",
    "gpt_info[\"domain\"] = get_num_featues(\"domain\", processed_data) / gpt_info[\"n\"] * 100\n",
    "gpt_info[\"tags\"] = get_num_featues(\"metatags\", processed_data) / gpt_info[\"n\"] * 100\n",
    "gpt_info[\"titles\"] = get_num_featues(\"title\", processed_data) /  gpt_info[\"n\"] * 100\n",
    "gpt_info[\"descriptions\"] = get_num_featues(\"description\", processed_data) / gpt_info[\"n\"] * 100\n",
    "gpt_info[\"keywords\"] = get_num_featues(\"keywords\", processed_data) / gpt_info[\"n\"] * 100\n",
    "gpt_info[\"links\"] = get_num_featues(\"links\", processed_data) / gpt_info[\"n\"] * 100\n",
    "gpt_info[\"sentences\"] = get_num_featues(\"sentences\", processed_data) / gpt_info[\"n\"] * 100\n",
    "\n",
    "for k, v in gpt_info.items():\n",
    "    if k != \"n\":\n",
    "        print(f\"ℹ️ Percentage of sites with {k}: {v:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curlie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 10000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wid</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>917678</td>\n",
       "      <td>www.winandsoft.fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>443072</td>\n",
       "      <td>gaude-ag.de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>728091</td>\n",
       "      <td>www.housing.ucsb.edu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132596</td>\n",
       "      <td>www.daccad.nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>464355</td>\n",
       "      <td>www.rockhall.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      wid                   url\n",
       "0  917678     www.winandsoft.fr\n",
       "1  443072           gaude-ag.de\n",
       "2  728091  www.housing.ucsb.edu\n",
       "3  132596         www.daccad.nl\n",
       "4  464355      www.rockhall.com"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Raw data\n",
    "curlie_data = hydra.utils.instantiate(curlie_cfg.data)\n",
    "\n",
    "raw_data = curlie_data.get_raw_data()\n",
    "processed_data = curlie_data.get_processed_data()\n",
    "embedded_jdata = curlie_data.get_embeddings()\n",
    "\n",
    "print(f\"Total number of samples: {len(raw_data)}\")\n",
    "raw_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected data on ['title', 'description', 'keywords', 'links', 'sentences', 'metatags', 'tld', 'domain']\n",
      "\n",
      "Title: Logiciel bibliothèque, médiathèque, vidéothèque.\n",
      "Description: Logiciel bibliothèque  logiciel permettant le catalogage de tous types de documents ou supports pour les besoins d'une bibliothèque. Le logiciel bibliothèque vous garantit la gestion documentaire la plus efficace (livres, cartes, manuscrits, incunables, etc.).\n",
      "Keywords: ['logiciel', 'bibliotheque,logiciel', 'mediatheque,logiciel', 'gestion', 'bibliotheque,logiciel', 'gestion', 'mediatheque,logiciel,bibliotheque,mediatheque']\n",
      "Tags: ['description', 'copyright', 'robots', 'keywords', 'author']\n",
      "Domain: winandsoft\n",
      "TLD: fr\n"
     ]
    }
   ],
   "source": [
    "# Example of processed website\n",
    "wid = list(processed_data.keys())[0]\n",
    "data = processed_data[wid]\n",
    "\n",
    "print(f\"Collected data on {list(data.keys())}\")\n",
    "\n",
    "# Show some examples\n",
    "print(f\"\\nTitle: {data['title']}\")\n",
    "print(f\"Description: {data['description']}\")\n",
    "print(f\"Keywords: {data['keywords']}\")\n",
    "print(f\"Tags: {data['metatags']}\")\n",
    "print(f\"Domain: {data['domain']}\")\n",
    "print(f\"TLD: {data['tld']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ Percentage of sites with tld: 100.00%\n",
      "ℹ️ Percentage of sites with domain: 100.00%\n",
      "ℹ️ Percentage of sites with tags: 95.47%\n",
      "ℹ️ Percentage of sites with titles: 98.28%\n",
      "ℹ️ Percentage of sites with descriptions: 62.95%\n",
      "ℹ️ Percentage of sites with keywords: 27.29%\n",
      "ℹ️ Percentage of sites with links: 91.62%\n",
      "ℹ️ Percentage of sites with sentences: 99.03%\n"
     ]
    }
   ],
   "source": [
    "curlie_info = dict()\n",
    "\n",
    "# Save these into a dict\n",
    "curlie_info[\"n\"] = len(processed_data)\n",
    "curlie_info[\"tld\"] = get_num_featues(\"tld\", processed_data) / curlie_info[\"n\"] * 100\n",
    "curlie_info[\"domain\"] = get_num_featues(\"domain\", processed_data) / curlie_info[\"n\"] * 100\n",
    "curlie_info[\"tags\"] = get_num_featues(\"metatags\", processed_data) / curlie_info[\"n\"] * 100\n",
    "curlie_info[\"titles\"] = get_num_featues(\"title\", processed_data) / curlie_info[\"n\"] * 100\n",
    "curlie_info[\"descriptions\"] = get_num_featues(\"description\", processed_data) / curlie_info[\"n\"] * 100\n",
    "curlie_info[\"keywords\"] = get_num_featues(\"keywords\", processed_data) / curlie_info[\"n\"] * 100\n",
    "curlie_info[\"links\"] = get_num_featues(\"links\", processed_data) / curlie_info[\"n\"] * 100\n",
    "curlie_info[\"sentences\"] = get_num_featues(\"sentences\", processed_data) / curlie_info[\"n\"] * 100\n",
    "\n",
    "for k, v in curlie_info.items():\n",
    "    if k != \"n\":\n",
    "        print(f\"ℹ️ Percentage of sites with {k}: {v:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LaTeX Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>Curlie-gpt-10k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <td>761.00</td>\n",
       "      <td>9190.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tld (%)</th>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>domain (%)</th>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tags (%)</th>\n",
       "      <td>93.69</td>\n",
       "      <td>95.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>titles (%)</th>\n",
       "      <td>98.42</td>\n",
       "      <td>98.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Original  Curlie-gpt-10k\n",
       "n             761.00         9190.00\n",
       "tld (%)       100.00          100.00\n",
       "domain (%)    100.00          100.00\n",
       "tags (%)       93.69           95.47\n",
       "titles (%)     98.42           98.28"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put the data info into a dataframe\n",
    "df = pd.DataFrame([original_info, curlie_info], index=[\"Original\", \"Curlie-gpt-10k\"]).round(2)\n",
    "\n",
    "# Change all columns names to include (%)\n",
    "df.columns = [f\"{c} (%)\" if c != \"n\" else c for c in df.columns]\n",
    "\n",
    "df = df.T\n",
    "\n",
    "# Save the dataframe to a latex table\n",
    "position = \"!ht\"\n",
    "save_path = os.path.join(root_path, \"report\", \"tables\", \"feature_information.tex\")\n",
    "latex = df.to_latex(\n",
    "    caption=\"Percentage of websites with each feature accross our datasets.\", \n",
    "    label=\"tab:feature_information\",\n",
    "    escape=True,\n",
    "    float_format=\"%.2f\",\n",
    "    position=position\n",
    ")\n",
    "\n",
    "# Add \\centering right after \\begin{table}\n",
    "latex = latex.replace(\"\\\\begin{table}\" + f\"[{position}]\", \"\\\\begin{table}\" + f\"[{position}]\" + \"\\n\\\\centering\")\n",
    "with open(save_path, \"w\") as f:\n",
    "    f.write(latex)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labelers\n",
    "\n",
    "---\n",
    "\n",
    "There are multiple GPT labeler instances that can be used to label the data. The labelers are defined in the labelers module and are identified by a `context`, `model` and `fewshow` parameter. The `context` parameter defines the context that is used to prompt the model. The `model` parameter defines the model that is used to generate the labels. The `fewshot` parameter defines whether the model is trained in a few-shot manner or not.\n",
    "\n",
    "| Parameter | Variants | Description |\n",
    "| --- | --- | ---  |\n",
    "| `context` | `context1` | Uses the `tld`, `domain` and `metatags` as context |\n",
    "| | `context2` | Uses the `tld`, `domain`, `metatags`, `links` and `text` as context |\n",
    "| | `context3` | Uses the `tld`, `domain`, `metatags`, `links`, `text` and `images` as context |\n",
    "| `model` | `gpt3.5` | Uses GPT-3.5 (`gpt-3.5-turbo-1106`) |\n",
    "| | `gpt4` | Uses GPT-4 (`gpt-4-1106-preview`) |\n",
    "| `fewshot` | `fewshot` | Injects an example website and label into the system prompt |\n",
    "| | `zeroshot` | Does not inject any example website or label into the system prompt |\n",
    "\n",
    "We are considering all contexts and few-shot settings for the GPT-3.5 model, but only the `context2` for the GPT-4 model. Additionally, we have the `human` labeler which simply loads the annotatations for the `original` dataset from the [Homepage2Vec paper](https://arxiv.org/abs/1905.09786)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise configuration for all labelers\n",
    "gpt_labeler_names = [\"gpt3.5-zeroshot-context1\", \"gpt3.5-oneshot-context1\", \"gpt3.5-zeroshot-context2\", \"gpt3.5-oneshot-context2\", \"gpt3.5-zeroshot-context3\", \"gpt3.5-oneshot-context3\"] \n",
    "\n",
    "gpt_labelers_cfg = {labeler: hydra.compose(config_name=\"eda\", overrides=[f\"labeler={labeler}\"]) for labeler in gpt_labeler_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate data\n",
    "original_data_cfg = hydra.compose(config_name=\"eda\", overrides=[\"data=original\"])\n",
    "original_data = hydra.utils.instantiate(original_data_cfg.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate labelers\n",
    "gpt_labelers = {labeler: hydra.utils.instantiate(cfg.labeler, data=original_data) for labeler, cfg in gpt_labelers_cfg.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instatntiate labelers dataframe\n",
    "def get_info(labeler):\n",
    "    return {\n",
    "        \"model\": labeler.model,\n",
    "        \"context\": labeler.name.split(\"-\")[-1],\n",
    "        \"shot\": labeler.fewshot,\n",
    "    }\n",
    "\n",
    "labeler_info = pd.DataFrame([get_info(labeler) for labeler in gpt_labelers.values()], index=gpt_labelers.keys())\n",
    "labeler_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that the labelers are working as expected by checking the number of labeled webpages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_processed_websites = len(original_data.get_processed_data())\n",
    "\n",
    "print(f\"ℹ️ Number of processed websites: {num_processed_websites}\")\n",
    "for name, labeler in gpt_labelers.items():\n",
    "    num_labels = len(labeler.get_labels())\n",
    "    print(f\"ℹ️ Number of {name} labels: {num_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling statistics\n",
    "\n",
    "Let's investigate some statistics about the labelers. We will compute:\n",
    "\n",
    "* The average number of labels per website\n",
    "* The number of valid labels\n",
    "* The number of invalid labels\n",
    "* The average time taken to label a website\n",
    "* The average number of prompt and completion tokens used to label a website\n",
    "* The estimated cost of labeling the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics(labeler):\n",
    "    statistics = {\"valid\": 0, \"invalid\": 0, \"lpp\": [], \"durations\": [], \"prompt_tokens\": [], \"completion_tokens\": []}\n",
    "    for website in labeler.get_labels().values():\n",
    "        if not website[\"is_valid\"]:\n",
    "            statistics[\"invalid\"] += 1\n",
    "            continue\n",
    "\n",
    "        statistics[\"valid\"] += 1\n",
    "        statistics[\"lpp\"].append(sum(website[\"labels\"]))\n",
    "        statistics[\"durations\"].append(website[\"duration\"])\n",
    "        statistics[\"prompt_tokens\"].append(website[\"prompt_tokens\"])\n",
    "        statistics[\"completion_tokens\"].append(website[\"completion_tokens\"])\n",
    "\n",
    "    lpps = np.array(statistics[\"lpp\"])\n",
    "    durations = np.array(statistics[\"durations\"])\n",
    "    prompt_tokens = np.array(statistics[\"prompt_tokens\"])\n",
    "    completion_tokens = np.array(statistics[\"completion_tokens\"])\n",
    "\n",
    "    statistics[\"lpp\"] = f\"{lpps.mean():.2f} ± {lpps.std():.2f}\"\n",
    "    statistics[\"durations\"] = f\"{durations.mean():.2f} ± {durations.std():.2f}\"\n",
    "    statistics[\"prompt_tokens\"] = f\"{prompt_tokens.mean():.2f} ± {prompt_tokens.std():.2f}\"\n",
    "    statistics[\"completion_tokens\"] = f\"{completion_tokens.mean():.2f} ± {completion_tokens.std():.2f}\"\n",
    "    \n",
    "    # Compute estimated cost\n",
    "    model = labeler.model\n",
    "    COST_PER_INP_TOKEN = GPT4_COST_PER_INP_TOKEN if \"gpt4\" in model else GPT3_5_COST_PER_INP_TOKEN\n",
    "    COST_PER_OUT_TOKEN = GPT4_COST_PER_OUT_TOKEN if \"gpt4\" in model else GPT3_5_COST_PER_OUT_TOKEN\n",
    "\n",
    "    statistics[\"estimated_cost\"] = f\"{(prompt_tokens.sum() * COST_PER_INP_TOKEN + completion_tokens.sum() * COST_PER_OUT_TOKEN):.2f}\"\n",
    "\n",
    "    return statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeler_statistics = pd.DataFrame([get_statistics(labeler) for labeler in gpt_labelers.values()], index=gpt_labelers.keys())\n",
    "labeler_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist(labeler):\n",
    "    labels = np.array([website[\"labels\"] for website in labeler.get_labels().values()])\n",
    "    return {category: count for category, count in zip(categories, labels.sum(0))}\n",
    "\n",
    "dists = {name: get_dist(labeler) for name, labeler in gpt_labelers.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for labeler, class_dist in dists.items():\n",
    "    for category, count in class_dist.items():\n",
    "        rows.append({\"labeler\": labeler, \"category\": category, \"count\": count})\n",
    "\n",
    "labeling_dists = pd.DataFrame(rows)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "sns.barplot(\n",
    "    labeling_dists,\n",
    "    x=\"category\",\n",
    "    y=\"count\",\n",
    "    hue=\"labeler\",\n",
    "    ax=ax\n",
    ")\n",
    "ax.get_legend().set_title(\"Labeler\")\n",
    "ax.set_xlabel(\"Category\", fontsize=14)\n",
    "ax.set_ylabel(\"Count\", fontsize=14)\n",
    "ax.set_title(\"Label Distribution per Category\", fontsize=16)\n",
    "\n",
    "print(f\"✅ Plotted labelled category distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling Quality\n",
    "\n",
    "The goal of all GPT labelers is to replicate the ground truth labels provide by the human annotators as closely as possible. As we only have human annotations for the original dataset, we can only evaluate the labelers on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instatiate human labeler\n",
    "original_cfg = hydra.compose(config_name=\"eda\", overrides=[\"data=original\"])\n",
    "human_labeler_cfg = hydra.compose(config_name=\"eda\", overrides=[\"labeler=human\", \"data=original\"])\n",
    "original_data = hydra.utils.instantiate(original_cfg.data)\n",
    "human_labeler = hydra.utils.instantiate(human_labeler_cfg.labeler, data=original_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_labels(labeler1, labeler2, subset = None):\n",
    "    labels1 = labeler1.get_labels()\n",
    "    labels2 = labeler2.get_labels()\n",
    "    wid1 = set(labels1.keys())\n",
    "    wid2 = set(labels2.keys())\n",
    "    matched_wid = wid1 & wid2\n",
    "    if subset:\n",
    "        matched_wid = matched_wid & subset\n",
    "\n",
    "    labels1 = np.array([labels1[wid][\"labels\"] for wid in matched_wid])\n",
    "    labels2 = np.array([labels2[wid][\"labels\"] for wid in matched_wid])\n",
    "\n",
    "    return labels1, labels2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "labeler_perf = []\n",
    "for name, labeler in gpt_labelers.items():\n",
    "    labels1, labels2 = match_labels(human_labeler, labeler)\n",
    "\n",
    "    acc = accuracy_score(labels1.flatten(), labels2.flatten())\n",
    "    subset_acc = accuracy_score(labels1, labels2)\n",
    "    macro_f1 = f1_score(labels1, labels2, average=\"macro\")\n",
    "    micro_f1 = f1_score(labels1, labels2, average=\"micro\")\n",
    "    weighted_f1 = f1_score(labels1, labels2, average=\"weighted\")\n",
    "    macro_precision = precision_score(labels1, labels2, average=\"macro\", zero_division=0)\n",
    "    micro_precision = precision_score(labels1, labels2, average=\"micro\", zero_division=0)\n",
    "    macro_recall = recall_score(labels1, labels2, average=\"macro\", zero_division=0)\n",
    "    micro_recall = recall_score(labels1, labels2, average=\"micro\", zero_division=0)\n",
    "\n",
    "    labeler_perf.append({\n",
    "        \"acc\": acc,\n",
    "        \"subset_acc\": subset_acc,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"micro_f1\": micro_f1,\n",
    "        \"weighted_f1\": weighted_f1,\n",
    "        \"macro_precision\": macro_precision,\n",
    "        \"micro_precision\": micro_precision,\n",
    "        \"macro_recall\": macro_recall,\n",
    "        \"micro_recall\": micro_recall,\n",
    "    })\n",
    "\n",
    "labeler_perf = pd.DataFrame(labeler_perf, index=gpt_labelers.keys())\n",
    "labeler_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join with labeling statistics\n",
    "labelers_df = pd.concat([labeler_info, labeler_statistics, labeler_perf], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-k performing labeler\n",
    "top_k = 5\n",
    "labelers_df.sort_values(\"macro_f1\", ascending=False).head(top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate performance\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(20, 5))\n",
    "xs = [\"model\", \"context\", \"shot\"]\n",
    "\n",
    "for ax, x in zip(axs, xs):\n",
    "    sns.barplot(\n",
    "        data=labelers_df,\n",
    "        x=x,\n",
    "        y=\"macro_f1\",\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_xlabel(x.capitalize(), fontsize=14)\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "axs[0].set_ylabel(\"Macro F1\", fontsize=14)\n",
    "\n",
    "print(f\"✅ Plotted labeler as function of model, context and shot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeler parameters\n",
    "params = [\"model\", \"context\", \"shot\"]\n",
    "\n",
    "rename_dict = {\n",
    "    \"gpt-3.5-turbo-1106\": \"GPT-3.5\",\n",
    "    \"gpt-4-1106-preview\": \"GPT-4\",\n",
    "    \"context1\": \"C1\",\n",
    "    \"context2\": \"C2\",\n",
    "    \"context3\": \"C3\",\n",
    "    \"shot\": \"Shot\",\n",
    "    \"model\": \"Model\",\n",
    "    \"context\": \"Context\",\n",
    "}\n",
    "\n",
    "fig = utils.grid(labelers_df, params, metric=\"mean\", cmap=\"YlGn\", figsize=(10, 10), rename_dict=rename_dict) \n",
    "print(f\"✅ Plotted figure.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-project-2-mlp-a6NSXBdT-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
