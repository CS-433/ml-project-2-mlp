{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noqa: E402\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os \n",
    "import sys\n",
    "\n",
    "import rootutils\n",
    "import hydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize hydra on every run\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "h = hydra.initialize(config_path=\"../conf\", job_name=\"eda\", version_base=None)\n",
    "\n",
    "# Setup root environment\n",
    "root_path = rootutils.setup_root(\".\")\n",
    "rootutils.set_root(\n",
    "    path=root_path,\n",
    "    project_root_env_var=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Websites\n",
    "\n",
    "---\n",
    "\n",
    "There are three copora of websites in this dataset:\n",
    "\n",
    "* `original`: 770 websites from the crowdsourced dataset from the [Homepage2Vec paper](https://arxiv.org/abs/1905.09786)\n",
    "* `gpt`: 250 common websites obtained by prompting GPT-4 (see [Prompts](https://chat.openai.com/share/a76c8b9b-a659-4b15-9ab0-d94af4733d58))\n",
    "* `curlie`: A filtered version of the [curlie](https://curlie.org) dataset, containing ~1M websites\n",
    "\n",
    "For each website, the repository contains a CSV file at the path `data/raw/<corpus>.csv` with the two columns - `wid` and `url`. The `wid` is a unique identifier for the website, and the `url` is the URL of the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Config\n",
    "original_cfg = hydra.compose(config_name=\"eda\", overrides=[\"data=original\"])\n",
    "gpt_cfg = hydra.compose(config_name=\"eda\", overrides=[\"data=gpt\"])\n",
    "curlie_cfg = hydra.compose(config_name=\"eda\", overrides=[\"data=curlie\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Data\n",
    "\n",
    "This is the data that was used to test the model in the original paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data\n",
    "original_data = hydra.utils.instantiate(original_cfg.data)\n",
    "\n",
    "raw_data = original_data.get_raw_data()\n",
    "processed_data = original_data.get_processed_data()\n",
    "embedded_data = original_data.get_embeddings()\n",
    "\n",
    "print(f\"Total number of samples: {len(raw_data)}\")\n",
    "raw_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of processed website\n",
    "wid = list(processed_data.keys())[0]\n",
    "data = processed_data[wid]\n",
    "\n",
    "print(f\"Collected data on {list(data.keys())}\")\n",
    "\n",
    "# Show some examples\n",
    "print(f\"\\nTitle: {data['title']}\")\n",
    "print(f\"Description: {data['description']}\")\n",
    "print(f\"Keywords: {data['keywords']}\")\n",
    "print(f\"Tags: {data['metatags']}\")\n",
    "print(f\"Domain: {data['domain']}\")\n",
    "print(f\"TLD: {data['tld']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_featues(feat):\n",
    "    return len([w[feat] for w in processed_data.values() if w[feat] is not None and w[feat] != []])\n",
    "\n",
    "n = len(processed_data)\n",
    "n_tld = get_num_featues(\"tld\")\n",
    "n_domain = get_num_featues(\"domain\")\n",
    "n_tags = get_num_featues(\"metatags\")\n",
    "n_titles = get_num_featues(\"title\")\n",
    "n_descriptions = get_num_featues(\"description\")\n",
    "n_keywords = get_num_featues(\"keywords\")\n",
    "n_links = get_num_featues(\"links\")\n",
    "\n",
    "# Print results\n",
    "print(f\"ℹ️ Number of sites with TLD: {n_tld/n*100:.2f}%\")\n",
    "print(f\"ℹ️ Number of sites with domain: {n_domain/n*100:.2f}%\")\n",
    "print(f\"ℹ️ Number of sites with tags: {n_tags/n*100:.2f}%\")\n",
    "print(f\"ℹ️ Number of sites with title: {n_titles/n*100:.2f}%\")\n",
    "print(f\"ℹ️ Number of sites with description: {n_descriptions/n*100:.2f}%\")\n",
    "print(f\"ℹ️ Number of sites with keywords: {n_keywords/n*100:.2f}%\")\n",
    "print(f\"ℹ️ Number of sites with links: {n_links/n*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data\n",
    "gpt_data = hydra.utils.instantiate(gpt_cfg.data)\n",
    "\n",
    "raw_data = gpt_data.get_raw_data()\n",
    "processed_data = gpt_data.get_processed_data()\n",
    "embedded_jdata = gpt_data.get_embeddings()\n",
    "\n",
    "print(f\"Total number of samples: {len(raw_data)}\")\n",
    "raw_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of processed website\n",
    "wid = list(processed_data.keys())[0]\n",
    "data = processed_data[wid]\n",
    "\n",
    "print(f\"Collected data on {list(data.keys())}\")\n",
    "\n",
    "# Show some examples\n",
    "print(f\"\\nTitle: {data['title']}\")\n",
    "print(f\"Description: {data['description']}\")\n",
    "print(f\"Keywords: {data['keywords']}\")\n",
    "print(f\"Tags: {data['metatags']}\")\n",
    "print(f\"Domain: {data['domain']}\")\n",
    "print(f\"TLD: {data['tld']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_featues(feat):\n",
    "    return len([w[feat] for w in processed_data.values() if w[feat] is not None and w[feat] != []])\n",
    "\n",
    "n = len(processed_data)\n",
    "n_tld = get_num_featues(\"tld\")\n",
    "n_domain = get_num_featues(\"domain\")\n",
    "n_tags = get_num_featues(\"metatags\")\n",
    "n_titles = get_num_featues(\"title\")\n",
    "n_descriptions = get_num_featues(\"description\")\n",
    "n_keywords = get_num_featues(\"keywords\")\n",
    "n_links = get_num_featues(\"links\")\n",
    "\n",
    "# Print results\n",
    "print(f\"ℹ️ Number of sites with TLD: {n_tld/n*100:.2f}%\")\n",
    "print(f\"ℹ️ Number of sites with domain: {n_domain/n*100:.2f}%\")\n",
    "print(f\"ℹ️ Number of sites with tags: {n_tags/n*100:.2f}%\")\n",
    "print(f\"ℹ️ Number of sites with title: {n_titles/n*100:.2f}%\")\n",
    "print(f\"ℹ️ Number of sites with description: {n_descriptions/n*100:.2f}%\")\n",
    "print(f\"ℹ️ Number of sites with keywords: {n_keywords/n*100:.2f}%\")\n",
    "print(f\"ℹ️ Number of sites with links: {n_links/n*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curlie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data\n",
    "curlie_data = hydra.utils.instantiate(gpt_cfg.data)\n",
    "\n",
    "raw_data = curlie_data.get_raw_data()\n",
    "processed_data = curlie_data.get_processed_data()\n",
    "embedded_jdata = curlie_data.get_embeddings()\n",
    "\n",
    "print(f\"Total number of samples: {len(raw_data)}\")\n",
    "raw_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of processed website\n",
    "wid = list(processed_data.keys())[0]\n",
    "data = processed_data[wid]\n",
    "\n",
    "print(f\"Collected data on {list(data.keys())}\")\n",
    "\n",
    "# Show some examples\n",
    "print(f\"\\nTitle: {data['title']}\")\n",
    "print(f\"Description: {data['description']}\")\n",
    "print(f\"Keywords: {data['keywords']}\")\n",
    "print(f\"Tags: {data['metatags']}\")\n",
    "print(f\"Domain: {data['domain']}\")\n",
    "print(f\"TLD: {data['tld']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_featues(feat):\n",
    "    return len([w[feat] for w in processed_data.values() if w[feat] is not None and w[feat] != []])\n",
    "\n",
    "n = len(processed_data)\n",
    "n_tld = get_num_featues(\"tld\")\n",
    "n_domain = get_num_featues(\"domain\")\n",
    "n_tags = get_num_featues(\"metatags\")\n",
    "n_titles = get_num_featues(\"title\")\n",
    "n_descriptions = get_num_featues(\"description\")\n",
    "n_keywords = get_num_featues(\"keywords\")\n",
    "n_links = get_num_featues(\"links\")\n",
    "\n",
    "# Print results\n",
    "print(f\"ℹ️ Number of sites with TLD: {n_tld/n*100:.2f}%\")\n",
    "print(f\"ℹ️ Number of sites with domain: {n_domain/n*100:.2f}%\")\n",
    "print(f\"ℹ️ Number of sites with tags: {n_tags/n*100:.2f}%\")\n",
    "print(f\"ℹ️ Number of sites with title: {n_titles/n*100:.2f}%\")\n",
    "print(f\"ℹ️ Number of sites with description: {n_descriptions/n*100:.2f}%\")\n",
    "print(f\"ℹ️ Number of sites with keywords: {n_keywords/n*100:.2f}%\")\n",
    "print(f\"ℹ️ Number of sites with links: {n_links/n*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labelers\n",
    "\n",
    "---\n",
    "\n",
    "There are multiple GPT labeler instances that can be used to label the data. The labelers are defined in the labelers module. These are:\n",
    "\n",
    "* `human`: Uses human labels from the [Homepage2Vec paper](https://arxiv.org/abs/1905.09786) -- only works for the `original` corpus\n",
    "* `gpt-labeler1`: Uses information on the `tld`, `domain`, `metatags`\n",
    "* `gpt-labeler2`: Uses information like `gpt-labeler1` +  `title`, `description`, `keywords`\n",
    "* `gpt-labeler3`: Uses information like `gpt-labeler2` +  `links` and `text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-project-2-mlp-a6NSXBdT-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
