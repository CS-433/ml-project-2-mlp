{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Standard imports\n",
    "import os\n",
    "import json\n",
    "\n",
    "# External imports\n",
    "import hydra\n",
    "import rootutils\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from wandb.sdk.wandb_run import Run\n",
    "import wandb\n",
    "\n",
    "import ml_project_2_mlp.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize hydra on every run\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "h = hydra.initialize(config_path=\"../conf\", job_name=\"eda\", version_base=None)\n",
    "\n",
    "# Setup root environment\n",
    "root_path = rootutils.setup_root(\".\")\n",
    "rootutils.set_root(\n",
    "    path=root_path,\n",
    "    project_root_env_var=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global paths\n",
    "ROOT_DIR = root_path\n",
    "ARTIFACT_DIR = os.path.join(ROOT_DIR, \"artifacts\")\n",
    "FIGURE_DIR = os.path.join(ROOT_DIR, \"report\", \"figures\")\n",
    "TABLE_DIR = os.path.join(ROOT_DIR, \"report\", \"tables\")\n",
    "\n",
    "# Global settings\n",
    "SAVE = False\n",
    "\n",
    "os.makedirs(FIGURE_DIR, exist_ok=True)\n",
    "os.makedirs(TABLE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Labeling\n",
    "rename_dict = {\n",
    "    \"human\": \"Human\",\n",
    "    \"gpt3.5\": \"GPT-3.5\",\n",
    "    \"gpt4\": \"GPT-4\",\n",
    "    \"context1\": \"Context 1\",\n",
    "    \"context2\": \"Context 2\",\n",
    "    \"context3\": \"Context 3\",\n",
    "    \"zeroshot\": \"0-shot\",\n",
    "    \"oneshot\": \"1-shot\",\n",
    "    \"f1\": \"Macro F1\",\n",
    "    \"acc\": \"Acc\",\n",
    "    \"precision\": \"Precision\",\n",
    "    \"recall\": \"Recall\",\n",
    "    \"lpp\": \"Labels Per Page\",\n",
    "    \"Kids_and_Teens\": \"Kids & Teens\",\n",
    "}\n",
    "\n",
    "def rename(x):\n",
    "    return rename_dict.get(x, x)\n",
    "\n",
    "def get_labeler_name(name: str):\n",
    "    return \" + \".join([rename_dict.get(n, n) for n in name.split(\"-\")])\n",
    "\n",
    "def get_metric_name(name: str):\n",
    "    if \"/\" in name:\n",
    "        split, metric = name.split(\"/\")\n",
    "        return f\"{rename_dict.get(split, split)} {rename_dict.get(metric, metric)}\"\n",
    "    else:\n",
    "        return rename_dict.get(name, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load categories\n",
    "with open(os.path.join(\"..\", \"data\", \"meta\", \"categories.json\"), \"r\") as f:\n",
    "    categories_and_desc = json.load(f)\n",
    "\n",
    "categories, categories_desc = zip(*categories_and_desc.items())\n",
    "idx2categories = dict(enumerate(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Style and colors\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"gist_stern\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise W&B\n",
    "WANDB_PROJECT = \"homepage2vec\"\n",
    "WANDB_ENTITY = \"ml-project-2-mlp\"\n",
    "\n",
    "# Initialize W&B API\n",
    "api = wandb.Api()\n",
    "\n",
    "# Get all runs\n",
    "runs = api.runs(f\"{WANDB_ENTITY}/{WANDB_PROJECT}\")\n",
    "print(f\"✅ Loaded {len(runs)} runs from W&B ({WANDB_ENTITY}/{WANDB_PROJECT})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers\n",
    "\n",
    "---\n",
    "\n",
    "First, we define a series of helper functions which we use to extract the relevant information (configuration, hyperparameters, performance metrics, etc.) from the W&B runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "def extract_config(run: Run) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the config from a run.\n",
    "    \"\"\"\n",
    "    data_attr = [\"name\"]\n",
    "    labeler_attr = [\"name\", \"model\", \"fewshot\", \"features\"]\n",
    "    train_data = {f\"train_data_{k}\":v for k,v  in run.config.get(\"train_data\", {}).items() if k in data_attr}\n",
    "    test_data = {f\"test_data_{k}\":v for k,v  in run.config.get(\"test_data\", {}).items() if k in data_attr}\n",
    "    train_labeler = {f\"train_labeler_{k}\":v for k,v  in run.config.get(\"train_labeler\", {}).items() if k in labeler_attr}\n",
    "    test_labeler = {f\"test_labeler_{k}\":v for k,v  in run.config.get(\"test_labeler\", {}).items() if k in labeler_attr}\n",
    "    train_ratio = run.config.get(\"train_datamodule\", {}).get(\"data_split\", [None])[0]\n",
    "    val_ratio, test_ratio = run.config.get(\"test_datamodule\", {}).get(\"data_split\", [None, None, None])[1:]\n",
    "\n",
    "    config = {\"id\": run.id, \"name\": run.name, \"finetune\": run.config[\"finetune\"], \"train_ratio\": train_ratio, \"val_ratio\": val_ratio, \"test_ratio\": test_ratio, **train_data, **test_data, **train_labeler, **test_labeler}\n",
    "\n",
    "    return config\n",
    "\n",
    "def extract_hparams(run: Run) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the hparams from a run.\n",
    "    \"\"\"\n",
    "    hparams = {\n",
    "        \"lr\": run.config.get(\"model\", {}).get(\"optimizer\", {}).get(\"lr\", None),\n",
    "        \"weight_decay\": run.config.get(\"model\", {}).get(\"optimizer\", {}).get(\"weight_decay\", None),\n",
    "        \"scheduler_factor\": run.config.get(\"model\", {}).get(\"scheduler\", {}).get(\"factor\", None),\n",
    "        \"batch_size\": run.config.get(\"train_datamodule\", {}).get(\"batch_size\", None),\n",
    "    }\n",
    "    return hparams\n",
    "\n",
    "def extract_summary(run: Run, exclude:list[str] = [\"test/cm\", \"test/report\"]) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the summary from a run.\n",
    "    \"\"\"\n",
    "    summary = {k:v for k, v in run.summary.items() if not k.startswith(\"_\") and k not in exclude}\n",
    "    return summary\n",
    "\n",
    "def runs_to_df(runs: list[Run]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert a list of W&B runs to a dataframe.\n",
    "    \"\"\"\n",
    "    # Extract information from runs\n",
    "    rows = []\n",
    "    for run in runs[::-1]:\n",
    "        config = extract_config(run)\n",
    "        hparams = extract_hparams(run)\n",
    "        summary = extract_summary(run)\n",
    "        rows.append({**config, **hparams, **summary})\n",
    "\n",
    "    # Add multi-index\n",
    "    columns = list(config.keys()) + list(hparams.keys()) + list(summary.keys())\n",
    "    config_tuples = [(\"config\", k) for k in config.keys()]\n",
    "    hparams_tuples = [(\"hparams\", k) for k in hparams.keys()]\n",
    "    summary_tuples = [(\"summary\", k) for k in summary.keys()]\n",
    "\n",
    "    # Create dataframe\n",
    "    run_df = pd.DataFrame(rows, columns=columns)\n",
    "    run_df.columns = pd.MultiIndex.from_tuples(\n",
    "        config_tuples + hparams_tuples + summary_tuples,\n",
    "    )\n",
    "    run_df.set_index((\"config\", \"id\"), inplace=True)\n",
    "\n",
    "    return run_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_runs(df_runs: pd.DataFrame, split: str=\"val\", metric: str=\"f1\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get the best runs based on the validation metric for \n",
    "    each unique combination of data, labeler - specified \n",
    "    in the run name.\n",
    "\n",
    "    Args:\n",
    "        df_runs (pd.DataFrame): Dataframe of runs.\n",
    "        metric (str): Metric to sort on.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe of best runs.\n",
    "    \"\"\"\n",
    "    experiment_cols = [(\"config\", \"train_labeler_name\"), (\"config\", \"finetune\")]\n",
    "    unique_exps = df_runs[experiment_cols].drop_duplicates()\n",
    "    best_runs = []\n",
    "    for unique_exp in unique_exps.values:\n",
    "        is_unique_exp = (df_runs[experiment_cols] == unique_exp).all(axis=1)\n",
    "        best_exp_run = df_runs[is_unique_exp].sort_values((\"summary\", f\"{split}/{metric}\"), ascending=False).iloc[0]\n",
    "        best_runs.append(best_exp_run)\n",
    "\n",
    "    return pd.DataFrame(best_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_cm(run: Run) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts the test confusion matrix from a run.\n",
    "    \"\"\"\n",
    "    test_cm = json.loads(run.summary.get(\"test/cm\", None))\n",
    "    if test_cm is None:\n",
    "        return None\n",
    "    test_cm = pd.DataFrame.from_dict(test_cm)\n",
    "    test_cm[\"category\"].replace(idx2categories, inplace=True)\n",
    "    test_cm.set_index(\"category\", inplace=True)\n",
    "    return test_cm\n",
    "\n",
    "def get_test_report(run: Run) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts the test report from a run.\n",
    "    \"\"\"\n",
    "    test_report = json.loads(run.summary.get(\"test/report\", None))\n",
    "    if test_report is None:\n",
    "        return None\n",
    "    test_report = pd.DataFrame.from_dict(test_report)\n",
    "    mapper_ = {str(idx): category for idx, category in idx2categories.items()}\n",
    "    test_report[\"category\"] = test_report[\"category\"].map(lambda x: mapper_.get(x, x))\n",
    "    return test_report\n",
    "\n",
    "def get_test_targets(run: Run) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts the test predictions from a run.\n",
    "    \"\"\"\n",
    "    test_targets = json.loads(run.summary.get(\"test/targets\"))\n",
    "    return pd.DataFrame(test_targets, columns=categories)\n",
    "\n",
    "def get_test_preds(run: Run) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts the test predictions from a run.\n",
    "    \"\"\"\n",
    "    test_preds = json.loads(run.summary.get(\"test/preds\"))\n",
    "    return pd.DataFrame(test_preds, columns=categories)\n",
    "\n",
    "def get_test_probs(run: Run) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts the test probabilities from a run.\n",
    "    \"\"\"\n",
    "    test_probs = json.loads(run.summary.get(\"test/probs\"))\n",
    "    return pd.DataFrame(test_probs, columns=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_reports_df(runs: list[Run]) -> pd.DataFrame:\n",
    "    test_reports_df = pd.DataFrame()\n",
    "    for run in runs:\n",
    "        run_config = extract_config(run)\n",
    "        test_report = get_test_report(run)\n",
    "\n",
    "        for k, v in run_config.items():\n",
    "            test_report[k] = v\n",
    "\n",
    "        # Concatenate\n",
    "        test_reports_df = pd.concat([test_reports_df, test_report])\n",
    "\n",
    "    return test_reports_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results \n",
    "\n",
    "---\n",
    "\n",
    "In this section we are loading all the runs in the group of runs that fine-tuned on `curlie-gpt3.5-10k` and `curlie-gpt4-10k`, respectively. We aim to analyse and visualise the general performance of the models in comparison to the original Homepage2Vec model. Finally, we analyse the hyperparameter grid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter runs for Experiment 2\n",
    "GROUP = \"exp2-3\"\n",
    "\n",
    "runs = [run for run in runs if run.group == GROUP and run.state == \"finished\"]\n",
    "\n",
    "print(f\"✅ Loaded {len(runs)} runs from W&B ({WANDB_ENTITY}/{WANDB_PROJECT} - {GROUP})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dataframe\n",
    "runs_df = runs_to_df(runs)\n",
    "\n",
    "# Get best runs by validation macro F1\n",
    "best_runs_df = best_runs(runs_df, split=\"val\", metric=\"f1\")\n",
    "\n",
    "# Show best runs sorted by test macro F1\n",
    "best_runs_df.sort_values((\"summary\", \"test/f1\"), ascending=False).summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save copy of best runs dataframe\n",
    "df = best_runs_df.copy()\n",
    "\n",
    "index = [x.config[\"train_labeler_name\"] if x.config[\"finetune\"] else \"Pretrained\" for _, x in df.iterrows()]\n",
    "index = [get_labeler_name(x) for x in index]\n",
    "df = df.summary[[\"test/precision\", \"test/recall\", \"test/f1\", \"test/lpp\"]]\n",
    "\n",
    "df = df.set_index(pd.Index(index))\n",
    "\n",
    "cols = {\"test/precision\": \"Pr.\", \"test/recall\": \"Re.\", \"test/f1\": \"M.-F1\", \"test/lpp\": \"LPP\"}\n",
    "df = df[cols.keys()].rename(columns=cols)\n",
    "\n",
    "df[\"Pr.\"] = df[\"Pr.\"] * 100\n",
    "df[\"Re.\"] = df[\"Re.\"] * 100\n",
    "df[\"M.-F1\"] = df[\"M.-F1\"] * 100\n",
    "\n",
    "# Save the dataframe to a latex table\n",
    "position = \"!ht\"\n",
    "save_path = os.path.join(TABLE_DIR, \"finetune-results.tex\")\n",
    "latex = df.to_latex(\n",
    "    caption=\"TODO\", \n",
    "    label=\"tab:finetune-results\",\n",
    "    escape=True,\n",
    "    position=position,\n",
    "    multirow=True,\n",
    "    float_format=\"%.2f\",\n",
    "    multicolumn=True,\n",
    "    multicolumn_format=\"c\",\n",
    ")\n",
    "latex = latex.replace(\"\\\\begin{table}\" + f\"[{position}]\", \"\\\\begin{table}\" + f\"[{position}]\" + \"\\n\\\\centering\")\n",
    "latex = latex.replace(\"[t]\", \"[c]\")\n",
    "\n",
    "# Save table if specified\n",
    "if SAVE:\n",
    "    with open(save_path, \"w\") as f:\n",
    "        f.write(latex)\n",
    "    print(f\"✅ Saved table to {save_path}\")\n",
    "else:\n",
    "    print(f\"❌ Not saving table. If you want to save it, set SAVE=True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning Classwise F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best runs\n",
    "pretrained_runs = exp2_best_runs_df[exp2_best_runs_df[(\"config\", \"finetune\")] == False]\n",
    "finetuned_runs = exp2_best_runs_df[exp2_best_runs_df[(\"config\", \"finetune\")] == True]\n",
    "\n",
    "pretrained_run_ids = pretrained_runs.index.values\n",
    "finetuned_run_ids = finetuned_runs.index.values\n",
    "\n",
    "pretrained_runs = [run for run in exp2_runs if run.id in pretrained_run_ids]\n",
    "finetuned_runs = [run for run in exp2_runs if run.id in finetuned_run_ids]\n",
    "\n",
    "print(f\"Got {len(pretrained_runs)} pretrained run(s) and {len(finetuned_runs)} finetuned run(s) for {GROUP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best runs\n",
    "pretrained_run = pretrained_runs[0]\n",
    "gpt3_5_run = finetuned_runs[0]\n",
    "gpt4_run = finetuned_runs[1]\n",
    "\n",
    "pretrained_cms = get_test_cm(pretrained_run)\n",
    "gpt3_5_cms = get_test_cm(gpt3_5_run)\n",
    "gpt4_cms = get_test_cm(gpt4_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the macro F1\n",
    "pretrained_report = get_test_report(pretrained_run)\n",
    "gpt3_5_report = get_test_report(gpt3_5_run)\n",
    "gpt4_report = get_test_report(gpt4_run)\n",
    "\n",
    "pretrained_report[\"model\"] = \"Pretrained\"\n",
    "gpt3_5_report[\"model\"] = \"GPT-3.5\"\n",
    "gpt4_report[\"model\"] = \"GPT-4\"\n",
    "\n",
    "test_reports = pd.concat([pretrained_report, gpt3_5_report, gpt4_report])\n",
    "\n",
    "test_reports = test_reports[test_reports[\"category\"].isin(categories)]\n",
    "test_reports[\"category\"] = test_reports[\"category\"].map(lambda x: rename(x))\n",
    "test_reports[\"f1-score\"] = test_reports[\"f1-score\"] * 100\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 4))\n",
    "fig.tight_layout(pad=3.0)\n",
    "sns.barplot(\n",
    "    data=test_reports,\n",
    "    x=\"category\",\n",
    "    y=\"f1-score\",\n",
    "    hue=\"model\",\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"F1 (%)\", fontsize=14)\n",
    "ax.set_xticks(ax.get_xticks())\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=30, horizontalalignment='right', fontsize=14)\n",
    "ax.get_legend().set_title(\"\")\n",
    "\n",
    "# Add values to bars\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    if height == 0:\n",
    "        continue\n",
    "    ax.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 1,\n",
    "            f\"{height:.0f}\",\n",
    "            ha=\"center\", fontsize=12)\n",
    "\n",
    "ax.set_ylim(0, 70)\n",
    "\n",
    "# Save figure\n",
    "path = os.path.join(FIGURE_DIR, \"finetune-results.pdf\")\n",
    "fig.savefig(path, bbox_inches=\"tight\")\n",
    "print(f\"✅ Saved figure to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the confusion matrices for the best runs\n",
    "fig, axs = plt.subplots(nrows=2, ncols=14, figsize=(4*14, 3*2))\n",
    "for i, (_, pretrained_cm) in enumerate(pretrained_cms.iterrows()):\n",
    "    sns.heatmap(\n",
    "        [[pretrained_cm[\"tn\"], pretrained_cm[\"fp\"]], [pretrained_cm[\"fn\"], pretrained_cm[\"tp\"]]],\n",
    "        ax=axs[0][i]\n",
    "    )\n",
    "    axs[0][i].set_title(pretrained_cm.name)\n",
    "    axs[0][i].set_xlabel(\"Predicted\")\n",
    "    if i > 0:\n",
    "        axs[0][i].set_yticks([])\n",
    "        axs[0][i].set_ylabel(\"\")\n",
    "\n",
    "for i, (_, pretrained_cm) in enumerate(finetuned_cms.iterrows()):\n",
    "    sns.heatmap(\n",
    "        [[pretrained_cm[\"tn\"], pretrained_cm[\"fp\"]], [pretrained_cm[\"fn\"], pretrained_cm[\"tp\"]]],\n",
    "        ax=axs[1][i]\n",
    "    )\n",
    "    axs[1][i].set_xlabel(\"Predicted\")\n",
    "    if i > 0:\n",
    "        axs[1][i].set_yticks([])\n",
    "        axs[1][i].set_ylabel(\"\")\n",
    "\n",
    "axs[0][0].set_ylabel(\"Actual\", fontsize=14)\n",
    "axs[0][1].set_ylabel(\"Actual\", fontsize=14)\n",
    "\n",
    "path = os.path.join(FIGURE_DIR, \"exp2-cms.pdf\")\n",
    "fig.savefig(path, bbox_inches=\"tight\")\n",
    "print(f\"✅ Saved figure to {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe with hyperparameters and test/f1\n",
    "grid_df = runs_df[\"hparams\"].copy()\n",
    "grid_df[\"test/f1\"] = runs_df[\"summary\"][\"test/f1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "fig, axs = plt.subplots(ncols=4, figsize=(20, 5))\n",
    "params = [\"lr\", \"weight_decay\", \"scheduler_factor\", \"batch_size\"]\n",
    "\n",
    "for ax, x in zip(axs, params):\n",
    "    sns.histplot(\n",
    "        data=grid_df,\n",
    "        x=x,\n",
    "        y=\"test/f1\",\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_xticks(ax.get_xticks())\n",
    "    ax.set_xticklabels([rename_dict.get(t.get_text(), t.get_text()) for t in ax.get_xticklabels()])\n",
    "    ax.set_xlabel(x.capitalize(), fontsize=14)\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "axs[0].set_ylabel(\"Macro F1\", fontsize=14)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-project-2-mlp-rx2AOdW0-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
