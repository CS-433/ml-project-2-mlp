{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Standard imports\n",
    "import os\n",
    "import json\n",
    "\n",
    "# External imports\n",
    "import hydra\n",
    "import rootutils\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from wandb.sdk.wandb_run import Run\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize hydra on every run\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "h = hydra.initialize(config_path=\"../conf\", job_name=\"eda\", version_base=None)\n",
    "\n",
    "# Setup root environment\n",
    "root_path = rootutils.setup_root(\".\")\n",
    "rootutils.set_root(\n",
    "    path=root_path,\n",
    "    project_root_env_var=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global paths\n",
    "ROOT_DIR = root_path\n",
    "ARTIFACT_DIR = os.path.join(ROOT_DIR, \"artifacts\")\n",
    "FIGURE_DIR = os.path.join(ROOT_DIR, \"report\", \"figures\")\n",
    "TABLE_DIR = os.path.join(ROOT_DIR, \"report\", \"tables\")\n",
    "\n",
    "os.makedirs(FIGURE_DIR, exist_ok=True)\n",
    "os.makedirs(TABLE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Labeling\n",
    "rename_dict = {\n",
    "    \"human\": \"Human\",\n",
    "    \"gpt3.5-zeroshot-context1\": \"GPT-3.5 +\\n0-shot +\\nContext 1\",\n",
    "    \"gpt3.5-zeroshot-context2\": \"GPT-3.5 +\\n0-shot +\\nContext 2\",\n",
    "    \"gpt3.5-zeroshot-context3\": \"GPT-3.5 +\\n0-shot +\\nContext 3\",\n",
    "    \"gpt3.5-oneshot-context1\": \"GPT-3.5 +\\n1-shot +\\nContext 1\",\n",
    "    \"gpt3.5-oneshot-context2\": \"GPT-3.5 +\\n1-shot +\\nContext 2\",\n",
    "    \"gpt3.5-oneshot-context3\": \"GPT-3.5 +\\n1-shot +\\nContext 3\",\n",
    "    \"gpt4-zeroshot-context2\": \"GPT-4 +\\n0-shot +\\nContext 2\",\n",
    "    \"gpt4-oneshot-context2\": \"GPT-4 +\\n1-shot +\\nContext 2\",\n",
    "    \"f1\": \"Macro F1 (%)\",\n",
    "    \"acc\": \"Acc. (%)\",\n",
    "    \"test/f1\": \"Test Macro F1 (%)\",\n",
    "    \"test/acc\": \"Test Acc. (%)\",\n",
    "    \"test/precision\": \"Test Precision (%)\",\n",
    "    \"test/recall\": \"Test Recall (%)\",\n",
    "    \"test/lpp\": \"Labels Per Page\",\n",
    "}\n",
    "\n",
    "with open(os.path.join(\"..\", \"data\", \"meta\", \"categories.json\"), \"r\") as f:\n",
    "    categories_and_desc = json.load(f)\n",
    "\n",
    "categories, categories_desc = zip(*categories_and_desc.items())\n",
    "idx2categories = dict(enumerate(categories))\n",
    "\n",
    "def rename(x):\n",
    "    return rename_dict.get(x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Style and colors\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise W&B\n",
    "# Initialize wandb\n",
    "WANDB_PROJECT = \"homepage2vec\"\n",
    "WANDB_ENTITY = \"ml-project-2-mlp\"\n",
    "\n",
    "# Initialize W&B API\n",
    "api = wandb.Api()\n",
    "\n",
    "# Get all runs\n",
    "runs = api.runs(f\"{WANDB_ENTITY}/{WANDB_PROJECT}\")\n",
    "print(f\"✅ Loaded {len(runs)} runs from W&B ({WANDB_ENTITY}/{WANDB_PROJECT})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "def extract_config(run: Run) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the config from a run.\n",
    "    \"\"\"\n",
    "    data_attr = [\"name\"]\n",
    "    labeler_attr = [\"name\", \"model\", \"fewshot\", \"features\"]\n",
    "    train_data = {f\"train_data_{k}\":v for k,v  in run.config.get(\"train_data\", {}).items() if k in data_attr}\n",
    "    test_data = {f\"test_data_{k}\":v for k,v  in run.config.get(\"test_data\", {}).items() if k in data_attr}\n",
    "    train_labeler = {f\"train_labeler_{k}\":v for k,v  in run.config.get(\"train_labeler\", {}).items() if k in labeler_attr}\n",
    "    test_labeler = {f\"test_labeler_{k}\":v for k,v  in run.config.get(\"test_labeler\", {}).items() if k in labeler_attr}\n",
    "    train_ratio = run.config.get(\"train_datamodule\", {}).get(\"data_split\", [None])[0]\n",
    "    val_ratio, test_ratio = run.config.get(\"test_datamodule\", {}).get(\"data_split\", [None, None, None])[1:]\n",
    "\n",
    "    config = {\"id\": run.id, \"name\": run.name, \"finetune\": run.config[\"finetune\"], \"train_ratio\": train_ratio, \"val_ratio\": val_ratio, \"test_ratio\": test_ratio, **train_data, **test_data, **train_labeler, **test_labeler}\n",
    "\n",
    "    return config\n",
    "\n",
    "def extract_hparams(run: Run) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the hparams from a run.\n",
    "    \"\"\"\n",
    "    hparams = {\n",
    "        \"lr\": run.config.get(\"model\", {}).get(\"optimizer\", {}).get(\"lr\", None),\n",
    "        \"weight_decay\": run.config.get(\"model\", {}).get(\"optimizer\", {}).get(\"weight_decay\", None),\n",
    "        \"scheduler_factor\": run.config.get(\"model\", {}).get(\"scheduler\", {}).get(\"factor\", None),\n",
    "        \"batch_size\": run.config.get(\"datamodule\", {}).get(\"batch_size\", None),\n",
    "    }\n",
    "    return hparams\n",
    "\n",
    "def extract_summary(run: Run, exclude:list[str] = [\"test/cm\", \"test/report\"]) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the summary from a run.\n",
    "    \"\"\"\n",
    "    summary = {k:v for k, v in run.summary.items() if not k.startswith(\"_\") and k not in exclude}\n",
    "    return summary\n",
    "\n",
    "def runs_to_df(runs: list[Run]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert a list of W&B runs to a dataframe.\n",
    "    \"\"\"\n",
    "    # Extract information from runs\n",
    "    rows = []\n",
    "    for run in runs[::-1]:\n",
    "        config = extract_config(run)\n",
    "        hparams = extract_hparams(run)\n",
    "        summary = extract_summary(run)\n",
    "        rows.append({**config, **hparams, **summary})\n",
    "\n",
    "    # Add multi-index\n",
    "    columns = list(config.keys()) + list(hparams.keys()) + list(summary.keys())\n",
    "    config_tuples = [(\"config\", k) for k in config.keys()]\n",
    "    hparams_tuples = [(\"hparams\", k) for k in hparams.keys()]\n",
    "    summary_tuples = [(\"summary\", k) for k in summary.keys()]\n",
    "\n",
    "    # Create dataframe\n",
    "    run_df = pd.DataFrame(rows, columns=columns)\n",
    "    run_df.columns = pd.MultiIndex.from_tuples(\n",
    "        config_tuples + hparams_tuples + summary_tuples,\n",
    "    )\n",
    "    run_df.set_index((\"config\", \"id\"), inplace=True)\n",
    "\n",
    "    return run_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_runs(df_runs: pd.DataFrame, split: str=\"val\", metric: str=\"f1\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get the best runs based on the validation metric for \n",
    "    each unique combination of data, labeler - specified \n",
    "    in the run name.\n",
    "\n",
    "    Args:\n",
    "        df_runs (pd.DataFrame): Dataframe of runs.\n",
    "        metric (str): Metric to sort on.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe of best runs.\n",
    "    \"\"\"\n",
    "    experiment_cols = [(\"config\", \"train_labeler_name\"), (\"config\", \"finetune\")]\n",
    "    unique_exps = df_runs[experiment_cols].drop_duplicates()\n",
    "    best_runs = []\n",
    "    for unique_exp in unique_exps.values:\n",
    "        is_unique_exp = (df_runs[experiment_cols] == unique_exp).all(axis=1)\n",
    "        print(is_unique_exp)\n",
    "        best_exp_run = df_runs[is_unique_exp].sort_values((\"summary\", f\"{split}/{metric}\"), ascending=False).iloc[0]\n",
    "        best_runs.append(best_exp_run)\n",
    "\n",
    "    return pd.DataFrame(best_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Fine-tuning on `original`\n",
    "\n",
    "---\n",
    "\n",
    "In this experiment we are verifying the annotation quality of the various `labelers` (specified in `conf/labelers`) by fine-tuning models on the `original` dataset with the labels provided by each labeler. The training parameters are kept constant for all the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter runs for Experiment 1\n",
    "GROUP = \"exp1\"\n",
    "\n",
    "exp1_runs = [run for run in runs if run.group == GROUP]\n",
    "\n",
    "print(f\"✅ Loaded {len(exp1_runs)} runs from W&B ({WANDB_ENTITY}/{WANDB_PROJECT} - {GROUP})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dataframe\n",
    "exp1_runs_df = runs_to_df(exp1_runs)\n",
    "\n",
    "exp1_runs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1_best_runs_df = best_runs(exp1_runs_df)\n",
    "\n",
    "# Show best runs by F1\n",
    "exp1_best_runs_df.sort_values((\"summary\", \"test/f1\"), ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1_best_runs_df.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise best runs by metric\n",
    "metrics = [\"test/f1\", \"test/precision\", \"test/recall\", \"test/acc\", \"test/lpp\"]\n",
    "fig, axs = plt.subplots(nrows=5, figsize=(20, 20))\n",
    "fig.tight_layout(pad=3.0)\n",
    "\n",
    "for ax, metric in zip(axs, metrics):\n",
    "    sns.barplot(\n",
    "        data=exp1_best_runs_df,\n",
    "        x=exp1_best_runs_df[(\"config\", \"name\")],\n",
    "        y=(\"summary\", metric),\n",
    "        hue=(\"config\", \"finetune\"),\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_xticks(ax.get_xticks())\n",
    "    run_names = [x.get_text() for x in ax.get_xticklabels()]\n",
    "    rows = [exp1_best_runs_df[exp1_best_runs_df[(\"config\", \"name\")] == run_name].iloc[0] for run_name in run_names]\n",
    "    xtick_labels = [rename(row[(\"config\", \"train_labeler_name\")]) for row in rows]\n",
    "    ax.set_xticks(ax.get_xticks())\n",
    "    ax.set_xticklabels(xtick_labels)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(rename(metric), fontsize=14)\n",
    "    ax.get_legend().set_title(\"Is Finetuned?\")\n",
    "    ax.get_legend().set_visible(False)\n",
    "\n",
    "axs[0].get_legend().set_visible(True)\n",
    "axs[-1].set_xlabel(\"Labelers\", fontsize=14)\n",
    "\n",
    "# Save figure\n",
    "fig.savefig(os.path.join(FIGURE_DIR, \"exp1-mf1.pdf\"), dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"✅ Saved figure to {FIGURE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show performance in splits for labelers\n",
    "def pivot_df(runs):\n",
    "    # Pivot df such that all split gets a column for each metric\n",
    "    pivoted_rows = []\n",
    "    for _, row in runs.iterrows():\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            pivoted_row = {\n",
    "                **row[[\"config\", \"hparams\"]],\n",
    "                (\"summary\", \"split\"): split,\n",
    "                (\"summary\", \"f1\"): row[(\"summary\", f\"{split}/f1\")],\n",
    "                (\"summary\", \"acc\"): row[(\"summary\", f\"{split}/acc\")],\n",
    "                (\"summary\", \"precision\"): row[(\"summary\", f\"{split}/precision\")] if split == \"test\" else None,\n",
    "                (\"summary\", \"recall\"): row[(\"summary\", f\"{split}/recall\")] if split == \"test\" else None,\n",
    "                (\"summary\", \"lpp\"): row[(\"summary\", f\"{split}/lpp\")] if split == \"test\" else None,\n",
    "            }\n",
    "            pivoted_rows.append(pivoted_row)\n",
    "\n",
    "    df = pd.DataFrame(pivoted_rows)\n",
    "    df.columns = pd.MultiIndex.from_tuples(df.columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot dataframe\n",
    "pivoted_exp1_best_runs_df = pivot_df(exp1_best_runs_df)\n",
    "fig, axs = plt.subplots(nrows=2, figsize=(20, 10))\n",
    "fig.tight_layout(pad=3.0)\n",
    "\n",
    "metrics = [\"f1\", \"acc\"]\n",
    "for ax, metric in zip(axs, metrics):\n",
    "    sns.barplot(\n",
    "        data=pivoted_exp1_best_runs_df,\n",
    "        x=(\"config\", \"name\"),\n",
    "        y=(\"summary\", metric),\n",
    "        hue=(\"summary\", \"split\"),\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    run_names = [x.get_text() for x in ax.get_xticklabels()]\n",
    "    rows = [pivoted_exp1_best_runs_df[pivoted_exp1_best_runs_df[(\"config\", \"name\")] == run_name].iloc[0] for run_name in run_names]\n",
    "    xtick_labels = [rename(row[(\"config\", \"train_labeler_name\")]) for row in rows]\n",
    "    ax.set_xticks(ax.get_xticks())\n",
    "    ax.set_xticklabels(xtick_labels)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(rename(metric), fontsize=14)\n",
    "\n",
    "path = os.path.join(FIGURE_DIR, \"exp1-splits.pdf\")\n",
    "fig.savefig(path, dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"✅ Saved figure to {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category-wise performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_cm(run: Run) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts the test confusion matrix from a run.\n",
    "    \"\"\"\n",
    "    test_cm = json.loads(run.summary.get(\"test/cm\", None))\n",
    "    if test_cm is None:\n",
    "        return None\n",
    "    test_cm = pd.DataFrame.from_dict(test_cm)\n",
    "    test_cm[\"category\"].replace(idx2categories, inplace=True)\n",
    "    test_cm.set_index(\"category\", inplace=True)\n",
    "    return test_cm\n",
    "\n",
    "def get_test_report(run: Run) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts the test report from a run.\n",
    "    \"\"\"\n",
    "    test_report = json.loads(run.summary.get(\"test/report\", None))\n",
    "    if test_report is None:\n",
    "        return None\n",
    "    test_report = pd.DataFrame.from_dict(test_report)\n",
    "    mapper_ = {str(idx): category for idx, category in idx2categories.items()}\n",
    "    test_report[\"category\"] = test_report[\"category\"].map(lambda x: mapper_.get(x, x))\n",
    "    return test_report\n",
    "\n",
    "def get_test_reports_df(runs: list[Run]) -> pd.DataFrame:\n",
    "    test_reports_df = pd.DataFrame()\n",
    "    for run in runs:\n",
    "        run_config = extract_config(run)\n",
    "        test_report = get_test_report(run)\n",
    "\n",
    "        for k, v in run_config.items():\n",
    "            test_report[k] = v\n",
    "\n",
    "        # Concatenate\n",
    "        test_reports_df = pd.concat([test_reports_df, test_report])\n",
    "\n",
    "    return test_reports_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class-wise test-reports\n",
    "exp1_test_reports_df = get_test_reports_df(exp1_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "fig, axs = plt.subplots(nrows=3, figsize=(20, 10))\n",
    "subset = exp1_test_reports_df[exp1_test_reports_df[\"train_labeler_name\"] == \"human\"]\n",
    "metrics = [\"precision\", \"recall\", \"f1-score\"]\n",
    "for ax, metric in zip(axs, metrics):\n",
    "    sns.barplot(\n",
    "        data=subset,\n",
    "        x=\"category\",\n",
    "        y=metric,\n",
    "        hue=\"finetune\",\n",
    "        ax=ax\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Fine-tuning on `curlie-10000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter runs for Experiment 2\n",
    "GROUP = \"exp2\"\n",
    "\n",
    "exp2_runs = [run for run in runs if run.group == GROUP]\n",
    "\n",
    "print(f\"✅ Loaded {len(exp2_runs)} runs from W&B ({WANDB_ENTITY}/{WANDB_PROJECT} - {GROUP})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dataframe\n",
    "exp2_runs_df = runs_to_df(exp2_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2_runs_df.sort_values((\"summary\", \"test/f1\"), ascending=False).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2_best_runs_df = best_runs(exp2_runs_df, split=\"val\", metric=\"f1_best\")\n",
    "\n",
    "exp2_best_runs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot dataframe\n",
    "pivoted_exp2_best_runs_df = pivot_df(exp2_best_runs_df)\n",
    "fig, axs = plt.subplots(nrows=2, figsize=(20, 10))\n",
    "fig.tight_layout(pad=3.0)\n",
    "\n",
    "metrics = [\"f1\", \"acc\"]\n",
    "for ax, metric in zip(axs, metrics):\n",
    "    sns.barplot(\n",
    "        data=pivoted_exp2_best_runs_df,\n",
    "        x=(\"config\", \"name\"),\n",
    "        y=(\"summary\", metric),\n",
    "        hue=(\"summary\", \"split\"),\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    run_names = [x.get_text() for x in ax.get_xticklabels()]\n",
    "    rows = [pivoted_exp2_best_runs_df[pivoted_exp2_best_runs_df[(\"config\", \"name\")] == run_name].iloc[0] for run_name in run_names]\n",
    "    xtick_labels = [rename(row[(\"config\", \"train_labeler_name\")]) for row in rows]\n",
    "    ax.set_xticks(ax.get_xticks())\n",
    "    ax.set_xticklabels(xtick_labels)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(rename(metric), fontsize=14)\n",
    "\n",
    "path = os.path.join(FIGURE_DIR, \"exp1-splits.pdf\")\n",
    "fig.savefig(path, dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"✅ Saved figure to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise performance of pre-trained against fine-tuned models\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.barplot(\n",
    "    data=exp2_best_runs_df,\n",
    "    x=(\"config\", \"name\"),\n",
    "    y=(\"summary\", \"test/f1\"),\n",
    "    hue=(\"config\", \"finetune\"),\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(rename(metric), fontsize=14)\n",
    "ax.get_legend().set_title(\"Is Finetuned?\")\n",
    "ax.get_legend().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2_test_reports_df = get_test_reports_df(exp2_runs)\n",
    "\n",
    "exp2_test_reports_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-project-2-mlp-rx2AOdW0-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
