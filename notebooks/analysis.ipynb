{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Standard imports\n",
    "import os\n",
    "import json\n",
    "\n",
    "# External imports\n",
    "import hydra\n",
    "import rootutils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from wandb.sdk.wandb_run import Run\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize hydra on every run\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "h = hydra.initialize(config_path=\"../conf\", job_name=\"eda\", version_base=None)\n",
    "\n",
    "# Setup root environment\n",
    "root_path = rootutils.setup_root(\".\")\n",
    "rootutils.set_root(\n",
    "    path=root_path,\n",
    "    project_root_env_var=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global paths\n",
    "ROOT_DIR = root_path\n",
    "ARTIFACT_DIR = os.path.join(ROOT_DIR, \"artifacts\")\n",
    "FIGURE_DIR = os.path.join(ROOT_DIR, \"report\", \"figures\")\n",
    "TABLE_DIR = os.path.join(ROOT_DIR, \"report\", \"tables\")\n",
    "\n",
    "os.makedirs(FIGURE_DIR, exist_ok=True)\n",
    "os.makedirs(TABLE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Labeling\n",
    "rename_dict = {\n",
    "    \"human\": \"Human\",\n",
    "    \"gpt3.5\": \"GPT-3.5\",\n",
    "    \"gpt4\": \"GPT-4\",\n",
    "    \"context1\": \"Context 1\",\n",
    "    \"context2\": \"Context 2\",\n",
    "    \"context3\": \"Context 3\",\n",
    "    \"zeroshot\": \"0-shot\",\n",
    "    \"oneshot\": \"1-shot\",\n",
    "    \"f1\": \"Macro F1\",\n",
    "    \"acc\": \"Acc\",\n",
    "    \"precision\": \"Precision\",\n",
    "    \"recall\": \"Recall\",\n",
    "    \"lpp\": \"Labels Per Page\",\n",
    "    \"Kids_and_Teens\": \"Kids & Teens\",\n",
    "}\n",
    "\n",
    "def rename(x):\n",
    "    return rename_dict.get(x, x)\n",
    "\n",
    "def get_labeler_name(name: str):\n",
    "    return \" + \".join([rename_dict.get(n, n) for n in name.split(\"-\")])\n",
    "\n",
    "def get_metric_name(name: str):\n",
    "    if \"/\" in name:\n",
    "        split, metric = name.split(\"/\")\n",
    "        return f\"{rename_dict.get(split, split)} {rename_dict.get(metric, metric)}\"\n",
    "    else:\n",
    "        return rename_dict.get(name, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load categories\n",
    "with open(os.path.join(\"..\", \"data\", \"meta\", \"categories.json\"), \"r\") as f:\n",
    "    categories_and_desc = json.load(f)\n",
    "\n",
    "categories, categories_desc = zip(*categories_and_desc.items())\n",
    "idx2categories = dict(enumerate(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Style and colors\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise W&B\n",
    "# Initialize wandb\n",
    "WANDB_PROJECT = \"homepage2vec\"\n",
    "WANDB_ENTITY = \"ml-project-2-mlp\"\n",
    "\n",
    "# Initialize W&B API\n",
    "api = wandb.Api()\n",
    "\n",
    "# Get all runs\n",
    "runs = api.runs(f\"{WANDB_ENTITY}/{WANDB_PROJECT}\")\n",
    "print(f\"✅ Loaded {len(runs)} runs from W&B ({WANDB_ENTITY}/{WANDB_PROJECT})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "def extract_config(run: Run) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the config from a run.\n",
    "    \"\"\"\n",
    "    data_attr = [\"name\"]\n",
    "    labeler_attr = [\"name\", \"model\", \"fewshot\", \"features\"]\n",
    "    train_data = {f\"train_data_{k}\":v for k,v  in run.config.get(\"train_data\", {}).items() if k in data_attr}\n",
    "    test_data = {f\"test_data_{k}\":v for k,v  in run.config.get(\"test_data\", {}).items() if k in data_attr}\n",
    "    train_labeler = {f\"train_labeler_{k}\":v for k,v  in run.config.get(\"train_labeler\", {}).items() if k in labeler_attr}\n",
    "    test_labeler = {f\"test_labeler_{k}\":v for k,v  in run.config.get(\"test_labeler\", {}).items() if k in labeler_attr}\n",
    "    train_ratio = run.config.get(\"train_datamodule\", {}).get(\"data_split\", [None])[0]\n",
    "    val_ratio, test_ratio = run.config.get(\"test_datamodule\", {}).get(\"data_split\", [None, None, None])[1:]\n",
    "\n",
    "    config = {\"id\": run.id, \"name\": run.name, \"finetune\": run.config[\"finetune\"], \"train_ratio\": train_ratio, \"val_ratio\": val_ratio, \"test_ratio\": test_ratio, **train_data, **test_data, **train_labeler, **test_labeler}\n",
    "\n",
    "    return config\n",
    "\n",
    "def extract_hparams(run: Run) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the hparams from a run.\n",
    "    \"\"\"\n",
    "    hparams = {\n",
    "        \"lr\": run.config.get(\"model\", {}).get(\"optimizer\", {}).get(\"lr\", None),\n",
    "        \"weight_decay\": run.config.get(\"model\", {}).get(\"optimizer\", {}).get(\"weight_decay\", None),\n",
    "        \"scheduler_factor\": run.config.get(\"model\", {}).get(\"scheduler\", {}).get(\"factor\", None),\n",
    "        \"batch_size\": run.config.get(\"train_datamodule\", {}).get(\"batch_size\", None),\n",
    "    }\n",
    "    return hparams\n",
    "\n",
    "def extract_summary(run: Run, exclude:list[str] = [\"test/cm\", \"test/report\"]) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the summary from a run.\n",
    "    \"\"\"\n",
    "    summary = {k:v for k, v in run.summary.items() if not k.startswith(\"_\") and k not in exclude}\n",
    "    return summary\n",
    "\n",
    "def runs_to_df(runs: list[Run]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert a list of W&B runs to a dataframe.\n",
    "    \"\"\"\n",
    "    # Extract information from runs\n",
    "    rows = []\n",
    "    for run in runs[::-1]:\n",
    "        config = extract_config(run)\n",
    "        hparams = extract_hparams(run)\n",
    "        summary = extract_summary(run)\n",
    "        rows.append({**config, **hparams, **summary})\n",
    "\n",
    "    # Add multi-index\n",
    "    columns = list(config.keys()) + list(hparams.keys()) + list(summary.keys())\n",
    "    config_tuples = [(\"config\", k) for k in config.keys()]\n",
    "    hparams_tuples = [(\"hparams\", k) for k in hparams.keys()]\n",
    "    summary_tuples = [(\"summary\", k) for k in summary.keys()]\n",
    "\n",
    "    # Create dataframe\n",
    "    run_df = pd.DataFrame(rows, columns=columns)\n",
    "    run_df.columns = pd.MultiIndex.from_tuples(\n",
    "        config_tuples + hparams_tuples + summary_tuples,\n",
    "    )\n",
    "    run_df.set_index((\"config\", \"id\"), inplace=True)\n",
    "\n",
    "    return run_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_runs(df_runs: pd.DataFrame, split: str=\"val\", metric: str=\"f1\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get the best runs based on the validation metric for \n",
    "    each unique combination of data, labeler - specified \n",
    "    in the run name.\n",
    "\n",
    "    Args:\n",
    "        df_runs (pd.DataFrame): Dataframe of runs.\n",
    "        metric (str): Metric to sort on.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe of best runs.\n",
    "    \"\"\"\n",
    "    experiment_cols = [(\"config\", \"train_labeler_name\"), (\"config\", \"finetune\")]\n",
    "    unique_exps = df_runs[experiment_cols].drop_duplicates()\n",
    "    best_runs = []\n",
    "    for unique_exp in unique_exps.values:\n",
    "        is_unique_exp = (df_runs[experiment_cols] == unique_exp).all(axis=1)\n",
    "        best_exp_run = df_runs[is_unique_exp].sort_values((\"summary\", f\"{split}/{metric}\"), ascending=False).iloc[0]\n",
    "        best_runs.append(best_exp_run)\n",
    "\n",
    "    return pd.DataFrame(best_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Finetuning on `original`\n",
    "\n",
    "---\n",
    "\n",
    "In this experiment we are verifying the annotation quality of the various `labelers` (specified in `conf/labelers`) by finetuning models on the `original` dataset with the labels provided by each labeler. The training parameters are kept constant for all the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter runs for Experiment 1\n",
    "GROUP = \"exp1-2\"\n",
    "\n",
    "exp1_runs = [run for run in runs if run.group == GROUP]\n",
    "\n",
    "print(f\"✅ Loaded {len(exp1_runs)} runs from W&B ({WANDB_ENTITY}/{WANDB_PROJECT} - {GROUP})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dataframe\n",
    "exp1_runs_df = runs_to_df(exp1_runs)\n",
    "\n",
    "exp1_runs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1_best_runs_df = best_runs(exp1_runs_df)\n",
    "\n",
    "# Show best runs by F1\n",
    "exp1_best_runs_df.sort_values((\"summary\", \"test/f1\"), ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise best runs by metric\n",
    "metrics = [\"test/f1\", \"test/precision\", \"test/recall\", \"test/acc\", \"test/lpp\"]\n",
    "fig, axs = plt.subplots(nrows=5, figsize=(20, 20))\n",
    "fig.tight_layout(pad=3.0)\n",
    "\n",
    "for ax, metric in zip(axs, metrics):\n",
    "    sns.barplot(\n",
    "        data=exp1_best_runs_df,\n",
    "        x=exp1_best_runs_df[(\"config\", \"name\")],\n",
    "        y=(\"summary\", metric),\n",
    "        hue=(\"config\", \"finetune\"),\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_xticks(ax.get_xticks())\n",
    "    run_names = [x.get_text() for x in ax.get_xticklabels()]\n",
    "    rows = [exp1_best_runs_df[exp1_best_runs_df[(\"config\", \"name\")] == run_name].iloc[0] for run_name in run_names]\n",
    "    xtick_labels = [rename(row[(\"config\", \"train_labeler_name\")]) for row in rows]\n",
    "    ax.set_xticks(ax.get_xticks())\n",
    "    ax.set_xticklabels(xtick_labels)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(rename(metric), fontsize=14)\n",
    "    ax.get_legend().set_title(\"Is Finetuned?\")\n",
    "    ax.get_legend().set_visible(False)\n",
    "\n",
    "axs[0].get_legend().set_visible(True)\n",
    "axs[-1].set_xlabel(\"Labelers\", fontsize=14)\n",
    "\n",
    "# Save figure\n",
    "fig.savefig(os.path.join(FIGURE_DIR, \"exp1-mf1.pdf\"), dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"✅ Saved figure to {FIGURE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show performance in splits for labelers\n",
    "def pivot_df(runs):\n",
    "    # Pivot df such that all split gets a column for each metric\n",
    "    pivoted_rows = []\n",
    "    for _, row in runs.iterrows():\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            pivoted_row = {\n",
    "                **row[[\"config\", \"hparams\"]],\n",
    "                (\"summary\", \"split\"): split,\n",
    "                (\"summary\", \"f1\"): row[(\"summary\", f\"{split}/f1\")],\n",
    "                (\"summary\", \"acc\"): row[(\"summary\", f\"{split}/acc\")],\n",
    "                (\"summary\", \"precision\"): row[(\"summary\", f\"{split}/precision\")] if split == \"test\" else None,\n",
    "                (\"summary\", \"recall\"): row[(\"summary\", f\"{split}/recall\")] if split == \"test\" else None,\n",
    "                (\"summary\", \"lpp\"): row[(\"summary\", f\"{split}/lpp\")] if split == \"test\" else None,\n",
    "            }\n",
    "            pivoted_rows.append(pivoted_row)\n",
    "\n",
    "    df = pd.DataFrame(pivoted_rows)\n",
    "    df.columns = pd.MultiIndex.from_tuples(df.columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot dataframe\n",
    "pivoted_exp1_best_runs_df = pivot_df(exp1_best_runs_df)\n",
    "fig, axs = plt.subplots(nrows=2, figsize=(20, 10))\n",
    "fig.tight_layout(pad=3.0)\n",
    "\n",
    "metrics = [\"f1\", \"acc\"]\n",
    "for ax, metric in zip(axs, metrics):\n",
    "    sns.barplot(\n",
    "        data=pivoted_exp1_best_runs_df,\n",
    "        x=(\"config\", \"name\"),\n",
    "        y=(\"summary\", metric),\n",
    "        hue=(\"summary\", \"split\"),\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    run_names = [x.get_text() for x in ax.get_xticklabels()]\n",
    "    rows = [pivoted_exp1_best_runs_df[pivoted_exp1_best_runs_df[(\"config\", \"name\")] == run_name].iloc[0] for run_name in run_names]\n",
    "    xtick_labels = [rename(row[(\"config\", \"train_labeler_name\")]) for row in rows]\n",
    "    ax.set_xticks(ax.get_xticks())\n",
    "    ax.set_xticklabels(xtick_labels)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(rename(metric), fontsize=14)\n",
    "\n",
    "path = os.path.join(FIGURE_DIR, \"exp1-splits.pdf\")\n",
    "fig.savefig(path, dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"✅ Saved figure to {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category-wise performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def get_test_cm(run: Run) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts the test confusion matrix from a run.\n",
    "    \"\"\"\n",
    "    test_cm = json.loads(run.summary.get(\"test/cm\", None))\n",
    "    if test_cm is None:\n",
    "        return None\n",
    "    test_cm = pd.DataFrame.from_dict(test_cm)\n",
    "    test_cm[\"category\"].replace(idx2categories, inplace=True)\n",
    "    test_cm.set_index(\"category\", inplace=True)\n",
    "    return test_cm\n",
    "\n",
    "def get_test_report(run: Run) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts the test report from a run.\n",
    "    \"\"\"\n",
    "    test_report = json.loads(run.summary.get(\"test/report\", None))\n",
    "    if test_report is None:\n",
    "        return None\n",
    "    test_report = pd.DataFrame.from_dict(test_report)\n",
    "    mapper_ = {str(idx): category for idx, category in idx2categories.items()}\n",
    "    test_report[\"category\"] = test_report[\"category\"].map(lambda x: mapper_.get(x, x))\n",
    "    return test_report\n",
    "\n",
    "def get_test_targets(run: Run) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts the test predictions from a run.\n",
    "    \"\"\"\n",
    "    test_targets = json.loads(run.summary.get(\"test/targets\"))\n",
    "    return pd.DataFrame(test_targets, columns=categories)\n",
    "\n",
    "def get_test_preds(run: Run) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts the test predictions from a run.\n",
    "    \"\"\"\n",
    "    test_preds = json.loads(run.summary.get(\"test/preds\"))\n",
    "    return pd.DataFrame(test_preds, columns=categories)\n",
    "\n",
    "def get_test_probs(run: Run) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts the test probabilities from a run.\n",
    "    \"\"\"\n",
    "    test_probs = json.loads(run.summary.get(\"test/probs\"))\n",
    "    return pd.DataFrame(test_probs, columns=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_reports_df(runs: list[Run]) -> pd.DataFrame:\n",
    "    test_reports_df = pd.DataFrame()\n",
    "    for run in runs:\n",
    "        run_config = extract_config(run)\n",
    "        test_report = get_test_report(run)\n",
    "\n",
    "        for k, v in run_config.items():\n",
    "            test_report[k] = v\n",
    "\n",
    "        # Concatenate\n",
    "        test_reports_df = pd.concat([test_reports_df, test_report])\n",
    "\n",
    "    return test_reports_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(nrows=3, figsize=(20, 10))\n",
    "# subset = exp1_test_reports_df[exp1_test_reports_df[\"train_labeler_name\"] == \"human\"]\n",
    "# metrics = [\"precision\", \"recall\", \"f1-score\"]\n",
    "# for ax, metric in zip(axs, metrics):\n",
    "#     sns.barplot(\n",
    "#         data=subset,\n",
    "#         x=\"category\",\n",
    "#         y=metric,\n",
    "#         hue=\"finetune\",\n",
    "#         ax=ax\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Finetuning on `curlie-10000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter runs for Experiment 2\n",
    "GROUP = \"exp2-2\"\n",
    "\n",
    "exp2_runs = [run for run in runs if run.group == GROUP]\n",
    "\n",
    "print(f\"✅ Loaded {len(exp2_runs)} runs from W&B ({WANDB_ENTITY}/{WANDB_PROJECT} - {GROUP})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dataframe\n",
    "exp2_runs_df = runs_to_df(exp2_runs)\n",
    "exp2_best_runs_df = best_runs(exp2_runs_df)\n",
    "\n",
    "# Show best runs by F1\n",
    "exp2_best_runs_df.sort_values((\"summary\", \"test/f1\"), ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2_best_runs_df.summary[[\"test/f1\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table to LaTex\n",
    "df = exp2_best_runs_df.copy()\n",
    "\n",
    "index = [x.config[\"train_labeler_name\"] if x.config[\"finetune\"] else \"Pretrained\" for _, x in df.iterrows()]\n",
    "index = [get_labeler_name(x) for x in index]\n",
    "# hparams = df.hparams.where(df.config[\"finetune\"], \"N/A\")\n",
    "df = df.summary[[\"test/precision\", \"test/recall\", \"test/f1\", \"test/lpp\"]]\n",
    "\n",
    "df = df.set_index(pd.Index(index))\n",
    "\n",
    "cols = {\"test/precision\": \"Pr.\", \"test/recall\": \"Re.\", \"test/f1\": \"M.-F1\", \"test/lpp\": \"LPP\"}\n",
    "df = df[cols.keys()].rename(columns=cols)\n",
    "\n",
    "df[\"Pr.\"] = df[\"Pr.\"] * 100\n",
    "df[\"Re.\"] = df[\"Re.\"] * 100\n",
    "df[\"M.-F1\"] = df[\"M.-F1\"] * 100\n",
    "\n",
    "# Save the dataframe to a latex table\n",
    "position = \"!ht\"\n",
    "save_path = os.path.join(TABLE_DIR, \"finetune-results.tex\")\n",
    "latex = df.to_latex(\n",
    "    caption=\"TODO\", \n",
    "    label=\"tab:finetune-results\",\n",
    "    escape=True,\n",
    "    position=position,\n",
    "    multirow=True,\n",
    "    float_format=\"%.2f\",\n",
    "    multicolumn=True,\n",
    "    multicolumn_format=\"c\",\n",
    ")\n",
    "\n",
    "# Add \\centering right after \\begin{table}\n",
    "latex = latex.replace(\"\\\\begin{table}\" + f\"[{position}]\", \"\\\\begin{table}\" + f\"[{position}]\" + \"\\n\\\\centering\")\n",
    "latex = latex.replace(\"[t]\", \"[c]\")\n",
    "with open(save_path, \"w\") as f:\n",
    "    f.write(latex)\n",
    "print(f\"✅ Saved table to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best runs\n",
    "pretrained_runs = exp2_best_runs_df[exp2_best_runs_df[(\"config\", \"finetune\")] == False]\n",
    "finetuned_runs = exp2_best_runs_df[exp2_best_runs_df[(\"config\", \"finetune\")] == True]\n",
    "\n",
    "pretrained_run_ids = pretrained_runs.index.values\n",
    "finetuned_run_ids = finetuned_runs.index.values\n",
    "\n",
    "pretrained_runs = [run for run in exp2_runs if run.id in pretrained_run_ids]\n",
    "finetuned_runs = [run for run in exp2_runs if run.id in finetuned_run_ids]\n",
    "\n",
    "print(f\"Got {len(pretrained_runs)} pretrained run(s) and {len(finetuned_runs)} finetuned run(s) for {GROUP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best runs\n",
    "pretrained_run = pretrained_runs[0]\n",
    "finetuned_run = finetuned_runs[0]\n",
    "\n",
    "pretrained_cms = get_test_cm(pretrained_run)\n",
    "finetuned_cms = get_test_cm(finetuned_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the macro F1\n",
    "pretrained_report = get_test_report(pretrained_run)\n",
    "finetuned_report = get_test_report(finetuned_run)\n",
    "\n",
    "pretrained_report[\"model\"] = \"Pretrained\"\n",
    "finetuned_report[\"model\"] = \"GPT-3.5\"\n",
    "\n",
    "test_reports = pd.concat([pretrained_report, finetuned_report])\n",
    "\n",
    "test_reports[\"category\"] = test_reports[\"category\"].map(lambda x: rename(x))\n",
    "test_reports[\"f1-score\"] = test_reports[\"f1-score\"] * 100\n",
    "test_reports = test_reports[test_reports[\"category\"].isin(categories)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 4))\n",
    "fig.tight_layout(pad=3.0)\n",
    "sns.barplot(\n",
    "    data=test_reports,\n",
    "    x=\"category\",\n",
    "    y=\"f1-score\",\n",
    "    hue=\"model\",\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"F1 (%)\", fontsize=14)\n",
    "labels = [rename(x.get_text()) for x in ax.get_xticklabels()]\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=30, horizontalalignment='right', fontsize=14)\n",
    "ax.get_legend().set_title(\"\")\n",
    "\n",
    "# Add values to bars\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    if height == 0:\n",
    "        continue\n",
    "    ax.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 1,\n",
    "            f\"{height:.1f}\",\n",
    "            ha=\"center\", fontsize=12)\n",
    "\n",
    "ax.set_ylim(0, 70)\n",
    "\n",
    "# Save figure\n",
    "path = os.path.join(FIGURE_DIR, \"exp2-mf1.pdf\")\n",
    "fig.savefig(path, bbox_inches=\"tight\")\n",
    "print(f\"✅ Saved figure to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the precision-recall curve\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "y_test, y_prob = get_test_targets(finetuned_run), get_test_probs(finetuned_run)\n",
    "y_test, y_prob = y_test.values, y_prob.values\n",
    "train_ratio = np.array(\n",
    "    [\n",
    "        0.093,\n",
    "        0.276,\n",
    "        0.062,\n",
    "        0.017,\n",
    "        0.059,\n",
    "        0.015,\n",
    "        0.011,\n",
    "        0.011,\n",
    "        0.084,\n",
    "        0.043,\n",
    "        0.048,\n",
    "        0.074,\n",
    "        0.139,\n",
    "        0.068,\n",
    "    ]\n",
    ")\n",
    "num_samples = len(y_test)\n",
    "pos_samples = num_samples * train_ratio\n",
    "neg_samples = num_samples - pos_samples\n",
    "pos_ratio = neg_samples / pos_samples\n",
    "y_prob2 = y_prob / (y_prob + pos_ratio * (1-y_prob))\n",
    "for k in range(14):\n",
    "    pr, re, th = precision_recall_curve(y_test[:, k], y_prob[:, k])\n",
    "    pr2, re2, th = precision_recall_curve(y_test[:, k], y_prob2[:, k])\n",
    "    ax[0].plot(re, pr, label=idx2categories[k])\n",
    "    ax[1].plot(re2, pr2, label=idx2categories[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the confusion matrices for the best runs\n",
    "fig, axs = plt.subplots(nrows=2, ncols=14, figsize=(4*14, 3*2))\n",
    "for i, (_, pretrained_cm) in enumerate(pretrained_cms.iterrows()):\n",
    "    sns.heatmap(\n",
    "        [[pretrained_cm[\"tn\"], pretrained_cm[\"fp\"]], [pretrained_cm[\"fn\"], pretrained_cm[\"tp\"]]],\n",
    "        ax=axs[0][i]\n",
    "    )\n",
    "    axs[0][i].set_title(pretrained_cm.name)\n",
    "    axs[0][i].set_xlabel(\"Predicted\")\n",
    "    if i > 0:\n",
    "        axs[0][i].set_yticks([])\n",
    "        axs[0][i].set_ylabel(\"\")\n",
    "\n",
    "for i, (_, pretrained_cm) in enumerate(finetuned_cms.iterrows()):\n",
    "    sns.heatmap(\n",
    "        [[pretrained_cm[\"tn\"], pretrained_cm[\"fp\"]], [pretrained_cm[\"fn\"], pretrained_cm[\"tp\"]]],\n",
    "        ax=axs[1][i]\n",
    "    )\n",
    "    axs[1][i].set_xlabel(\"Predicted\")\n",
    "    if i > 0:\n",
    "        axs[1][i].set_yticks([])\n",
    "        axs[1][i].set_ylabel(\"\")\n",
    "\n",
    "axs[0][0].set_ylabel(\"Actual\", fontsize=14)\n",
    "axs[0][1].set_ylabel(\"Actual\", fontsize=14)\n",
    "\n",
    "path = os.path.join(FIGURE_DIR, \"exp2-cms.pdf\")\n",
    "fig.savefig(path, bbox_inches=\"tight\")\n",
    "print(f\"✅ Saved figure to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot dataframe\n",
    "pivoted_exp2_best_runs_df = pivot_df(exp2_best_runs_df)\n",
    "fig, axs = plt.subplots(nrows=2, figsize=(20, 10))\n",
    "fig.tight_layout(pad=3.0)\n",
    "\n",
    "metrics = [\"f1\", \"acc\"]\n",
    "for ax, metric in zip(axs, metrics):\n",
    "    sns.barplot(\n",
    "        data=pivoted_exp2_best_runs_df,\n",
    "        x=(\"config\", \"name\"),\n",
    "        y=(\"summary\", metric),\n",
    "        hue=(\"summary\", \"split\"),\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    run_names = [x.get_text() for x in ax.get_xticklabels()]\n",
    "    rows = [pivoted_exp2_best_runs_df[pivoted_exp2_best_runs_df[(\"config\", \"name\")] == run_name].iloc[0] for run_name in run_names]\n",
    "    xtick_labels = [rename(row[(\"config\", \"train_labeler_name\")]) for row in rows]\n",
    "    ax.set_xticks(ax.get_xticks())\n",
    "    ax.set_xticklabels(xtick_labels)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(rename(metric), fontsize=14)\n",
    "\n",
    "path = os.path.join(FIGURE_DIR, \"exp1-splits.pdf\")\n",
    "fig.savefig(path, dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"✅ Saved figure to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-project-2-mlp-rx2AOdW0-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
