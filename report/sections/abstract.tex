\begin{abstract}

This study explores the use of Large Language Models (LLMs) for creating a fine-tuning dataset to improve Homepage2Vec~\cite{homepage2vec}, a state-of-the-art model for multilingual, multilabel website classification. Addressing the single-label bias in the Curlie dataset used for initial training, we assess various LLM-based labelers and select the best one through comparison to crowdsourced annotations. We generate a new 10,000-website dataset, \texttt{curlie-gpt3.5-10k}, for fine-tuning Homepage2Vec. 
Our contributions encompass demonstrating the effectiveness of LLMs in obtaining high-quality annotations, enhancing Homepage2vec's performance from 38\% to 42\% through fine-tuning, and, finally, releasing \texttt{curlie-gpt3.5-10k} to foster further advancements in multilingual multilabel website classification research.

\end{abstract}