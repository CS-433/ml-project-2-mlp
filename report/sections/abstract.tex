\begin{abstract}
This study explores the application of Generative Pre-trained Transformers (GPT) models in automating the annotation process for multilingual multilabel website classification. 
Leveraging advancements in natural language processing and the potential of GPT models, we address the resource-intensive and time-consuming nature of labeling tasks. 
Focusing on the Homepage2vec \cite{homepage2vec} model trained on Curlie web directory data, our investigation aims to overcome limitations in single-label assignment prevalent in the open-source Curlie dataset. We utilize crowdsourced annotations for 800 Curlie websites to identify an optimal GPT labeler, fine-tune the Homepage2vec model, and evaluate its performance against human-annotated data. 
Our contributions encompass demonstrating the effectiveness of GPT models in obtaining high-quality annotations, enhancing Homepage2vec's classification performance through GPT-annotated data fine-tuning, and releasing the \texttt{curlie-10k} dataset to foster further advancements in multilingual multilabel website classification research.

\end{abstract}