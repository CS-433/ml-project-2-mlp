\section{Introduction}
The success of numerous machine learning projects relies heavily on the availability of well-labeled data. While obtaining vast amounts of unlabeled data is often straightforward, the labeling process can be resource-intensive and time-consuming. Recent advances in natural language processing (NLP), particularly the advent of transformer architectures \cite{transformers} and Generative Pre-trained Transformers (GPT) models \cite{gpt3, gpt4}, hold the potential to automate this laborious and costly task. Since the introduction of GPT-3 \cite{gpt3}, various studies have explored the capabilities of GPT models in optimizing annotation prompts \cite{prompt-tuning}, balancing labeling cost and quality \cite{reduce-labeling-cost}, leveraging dictionaries for labeling \cite{is-gpt3-good-annot}, and incorporating reasoning behind annotations \cite{annollm}.

In this study, we delve into the use of GPT models to generate high-quality annotations for the multilingual multilabel website classification task, enhancing the classification performance of a downstream model. Multilingual website classification is pivotal for web search engines, web crawlers used in index building, and research endeavors analyzing web content. Our baseline model is Homepage2vec, trained on publicly available data from the Curlie web directory \cite{homepage2vec}. While Homepage2vec exhibits promising results, its primary limitation stems from the majority of samples being assigned a single label, owing to the open-source nature of Curlie, lacking strict guidelines on label assignment.

The Homepage2vec authors released crowdsourced annotations for 800 Curlie websites. Leveraging these annotations, our work seeks to identify an optimal GPT labeler for subsequent labeling of a larger subset of the Curlie dataset. We employ this GPT-labeled data to fine-tune the existing Homepage2vec model and assess its performance against human-annotated crowdsourced data.

Given the above mentioned previous research directions, our work contributes in three key areas. Firstly, we demonstrate the use of GPT models to obtain high-quality annotations for multilingual mutlilabel website classification. Secondly, we enhance the Homepage2vec model's performance through fine-tuning on GPT-annotated data, making the model accessible to the research community. Lastly, we release our GPT-annotated dataset, GPT-Curlie-10k, facilitating further advancements in the Homepage2vec model.
