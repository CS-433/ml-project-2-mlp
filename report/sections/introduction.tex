\section{Introduction}

This study focuses on enhancing Homepage2Vec~\cite{homepage2vec}, a leading tool in multilingual website embeddings and topic classification, crucial for search engines, web crawlers, and large-scale web content analysis. While Homepage2Vec exhibits promising results, one of its major limitations stems from its training dataset, Curlie~\cite{curlie}. The website topics are assigned by volunteers without strict annotation guidelines or quality control mechanisms. This results in most websites being assigned only a single label. However, the authors of Homepage2Vec demonstrate that most websites are in fact associated with multiple topics, as verified by a crowdsourced re-annotation of a small subset of Curlie. We hypothesise that finetuning Homepage2Vec on a larger set of high-quality annotations can improve its performance.

Given the resource-intensive nature of manual re-annotation, we turn to advancements in natural language processing (NLP), particularly the emergence of Large Language Models (LLMs)~\cite{gpt3, gpt4} as a viable alternative for generating reliable annotations. Prior studies affirm the efficiency and quality of LLMs in annotation tasks, suggesting their potential in multilabel website topic classification~\cite{is-gpt3-good-annot,prompt-tuning,annollm,reduce-labeling-cost}.

In summary, our work contributes in three key areas. Firstly, we demonstrate the use of LLMs to obtain high-quality annotations for multilingual multilabel website classification. Secondly, we enhance Homepage2vec's performance through finetuning on LLM-annotated data. Lastly, we release two LLM-annotated datasets, \texttt{curlie-gpt3.5-10k} and \texttt{curlie-gpt4-10k}, facilitating further advancements in the field of multilingual website classification.

\textit{The code and experiments are available on \href{https://github.com/CS-433/ml-project-2-mlp}{GitHub} and \href{https://wandb.ai/ml-project-2-mlp/homepage2vec}{W\&B}.}
