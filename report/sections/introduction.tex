% Some important notes from the homepage2vec paper
% 1) Intro
% Before homepage2vec
% - No multilingual models
% - No embeddings based methods
% - Usually paid services
% With Homepage2vec
% - multilingual -> great because out of top 10M websites, 40% are not in English
% - embeddings based
% - open source
% - fast since you can run it locally, and do not have to use external API

% 2) Related work
% Related approaches
% - manual approaches back in the days
% - ML based methods that use contextual features of the given webpage
% - Methods that also use for context the surrounding webpages, especially useful when the current page does not include that much info
% - Methods that use vision features
% - Recently, use of the LSTM, BERT, GRU architectures -> shown to increase performance -> however focus purely on English

% Multilingual embeddings
% - They are using XML-R, multi-lingual model, that has shown to be comparable to the monolingual models

% 3) Dataset
% - Curlie = community edited web directory -> 3M websites in 92 languages,
% labeled in hierarchical categories, however they only used the top level categories
% Originally, there were 15 top level categories, but they dropped "Regional"
% - Majority of classes associated with Bussiness (27), Society (13.9) or Arts (9)
% - 40% of the websites are in English, 16 % in German, 5% in french, 6% in Japanese
% - Although each page may, in principle, have an arbitrary number of category labels, 
% at the top level, the data is mostly single-labeled, with only 2.1% of samples appearing 
% in two or more taxonomy trees of the 14 top-level classes.

% 4) Method
% - They embeded only the first 100 sentences since the embedding process is quite expensive
% This was selected based on the validation set performance using the elbow method
% - They use 19 most frequent domains exluding the domains that indicate country
% - Title, description and keywords are used as well and should be very informative
% - With the links, they use the anchor text and the 50 most frequent texts are used, again
% this was selected using the elbow method


\section{Introduction}
The success of numerous machine learning projects relies heavily on the availability of well-labeled data. While obtaining vast amounts of unlabeled data is often straightforward, the labeling process can be resource-intensive and time-consuming. Recent advances in natural language processing (NLP), particularly the advent of transformer architectures \cite{transformers} and Generative Pre-trained Transformers (GPT) models \cite{gpt3, gpt4}, hold the potential to automate this laborious and costly task. Since the introduction of GPT-3 \cite{gpt3}, various studies have explored the capabilities of GPT models in optimizing annotation prompts \cite{prompt-tuning}, balancing labeling cost and quality \cite{reduce-labeling-cost}, leveraging dictionaries for labeling \cite{is-gpt3-good-annot}, and incorporating reasoning behind annotations \cite{annollm}.

In this study, we delve into the use of GPT models to generate high-quality annotations for the multilingual multilabel website classification task, enhancing the classification performance of a downstream model. Multilingual website classification is pivotal for web search engines, web crawlers used in index building, and research endeavors analyzing web content. Our baseline model is Homepage2vec, trained on publicly available data from the Curlie web directory \cite{homepage2vec}. While Homepage2vec exhibits promising results, its primary limitation stems from the majority of samples being assigned a single label, owing to the open-source nature of Curlie, lacking strict guidelines on label assignment.

The Homepage2vec authors released crowdsourced annotations for 800 Curlie websites. Leveraging these annotations, our work seeks to identify an optimal GPT labeler for subsequent labeling of a larger subset of the Curlie dataset. We employ this GPT-labeled data to fine-tune the existing Homepage2vec model and assess its performance against human-annotated crowdsourced data.

Given the above mentioned previous research directions, our work contributes in three key areas. Firstly, we demonstrate the use of GPT models to obtain high-quality annotations for multilingual mutlilabel website classification. Secondly, we enhance the Homepage2vec model's performance through fine-tuning on GPT-annotated data, making the model accessible to the research community. Lastly, we release our GPT-annotated dataset, GPT-Curlie-10k, facilitating further advancements in the Homepage2vec model.
