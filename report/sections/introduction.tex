\section{Introduction}

The success of numerous machine learning projects relies heavily on the abundance of high-quality labels. However, the labeling process is generally resource-intensive, often necessitating meticulous planning, expert guidance, and the involvement of human annotators. Recent advances in natural language processing (NLP), particularly the advent of the Transformer architecture~\cite{transformers} and Generative Pre-trained Transformers (GPT) models~\cite{gpt3, gpt4}, hold the potential to automate this laborious and costly task. Since the introduction of GPT-3~\cite{gpt3}, various studies have shown that LLMs are capable of creating efficient, high-quality annotations in various settings~\cite{prompt-tuning,reduce-labeling-cost,is-gpt3-good-annot, annollm}.

This study focuses on enhancing Homepage2Vec~\cite{homepage2vec}, a leading tool in multilingual website embeddings and topic classification, crucial for search engines, web crawlers, and large-scale web content analysis. While Homepage2vec exhibits promising results, one of its major limitations is its training dataset, \texttt{curlie}~\cite{homepage2vec}. This dataset, a refined subset of the public Curlie web directory dataset~\textit{CITE}, inaccurately depicts the true per page topic distribution. Likely owing to the open-source nature of Curlie and the lack of consistent annotation guidelines, most websites are assigned only a single label. In contrast, the authors of Homepage2vec demonstrate that most websites are  associated with multiple topics, given crowdsourced annotations of 800 websites, collected in \texttt{crowdsourced}~\cite{homepage2vec}.

Given these limitations, our objective is to (1) identify an optimal LLM labeler for subsequent labeling of a larger subset of the Curlie dataset and (2) use this labeled data to fine-tune the existing Homepage2vec model. We hypothesise that LLMs can provide high-quality annotations, allowing for knowledge transfer to Homepage2vec.

In summary, our work contributes in three key areas. Firstly, we demonstrate the use of LLMs to obtain high-quality annotations for multilingual mutlilabel website classification. Secondly, we enhance Homepage2vec's performance through fine-tuning on LLM-annotated data, making the model accessible to the research community. Lastly, we release our LLM-annotated dataset, \texttt{curlie-gpt3.5-10k}, facilitating further advancements in the field of multilingual website classification.

\textit{The code and experiments are publicly available on \href{https://github.com/CS-433/ml-project-2-mlp}{GitHub} and \href{https://wandb.ai/ml-project-2-mlp/homepage2vec}{W\&B}.}
