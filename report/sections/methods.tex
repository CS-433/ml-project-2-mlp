\section{Methodology}\label{sec:methodology}

The overall goal of our work is to improve mutlilabel classification performance of the original Homepage2Vec model~\cite{homepage2vec}. We refer to this model as \texttt{baseline}. We can divide our work into two main phases: (1) identifying the best-performing LLM annotator and (2) fine-tuning the baseline model on a dataset annotated by the best-performing LLM annotator. In the following, we describe the methodology for each phase in detail.

\subsection*{Phase 1: LLM Labeling}

\input{tables/labeler-params.tex}

% Labeler Setup (Variants)
In the first phase, we aim to obtain high-quality LLM-generated annotations for the topics of a website. 
In our study we only consider GPT labelers queried via the OpenAI API, mainly due to the convenience and the control over the response format. However, in theory our methodology can be applied to any LLM labeler. We consider a total of 12 GPT labelers by varying the model version, context, and whether we include an example annotation in the prompt. Table~\ref{tab:labeler-params} shows the parameters and decriptions of each variant. Each unique parameter combination makes up a unique labeler.

% Details about parameters and variants
The context defines the amount of information about the website that is provided to the model in the system prompt and during the annotation process, and is inspired by the feature importance reported in the original Homepage2Vec paper~\cite{homepage2vec}. \texttt{context1} only uses information about the domain and meta-tags. \texttt{context2} adds the \texttt{title}, \texttt{description}, and \texttt{keywords}, and \texttt{context3} adds the first 100 sentences and 50 links. The example annotation provided if the few-shot flag is set is an annotation of the NY Times website, which was chosen because of the high number of labels that can be assigned to the website. The system prompt is kept constant across all labelers and is shown in Appendix~\ref{app:prompt}. The user prompt is a simple JSON dump of the context provided about the website to classify.

% Gold standard
To identify high-quality annotators, we use the dataset \texttt{crowdsourced} as our gold standard. The dataset was created by the authors of Homepage2Vec~\cite{homepage2vec} and contains 840 websites, each annotated by three human annotators. The measured inter-annotator agreement measured by the pairwise Cohen's kappa~\cite{cohen-coef} is $0.2 \pm 0.02$, indicating low agreement. We assign a category label if at least two annotators agree, resulting in an average of 2.5 labels per website.

% Evaluation
We obtain labels from all labelers by manually scraping and preprocessing the websites defined in \texttt{crowdsourced}. The scraping and processing pipeline is kept identical to the one used in Homepage2Vec to allow for comparison of the GPT labelers to the \texttt{baseline}. However, some websites could not be reached at the time of writing, limiting the evaluation of all annotators to 761 websites.

% Curlie-10k dataset
Finally, we use the GPT-3.5 and GPT-4 annotators that finds the best trade-off between cost and quality to annotate a random subset of 10,000 websites from the Curlie website directory. We will refer to these datasets as \texttt{curlie-gpt3.5-10k} and \texttt{curlie-gpt4-10k} respectively. The datasets are used in the second phase of our study to fine-tune the baseline model.

\subsection* {Phase 2: Knowledge Distillation}

In the second phase, we aim to transfer the knowledge from the LLMs into Homepage2Vec via fine-tuning.

% Training
Training is performed on the \texttt{curlie-gpt3.5-10k} and \texttt{curlie-gpt4-10k} dataset for a maximum of 100 epochs. We use a 20\% held-out validation split from the \texttt{crowdsourced} dataset
to monitor the validation F1 score and stop training if no improvement is observed for 20 epochs. This is to prevent overfitting the GPT labels. We perform extensive hyperparameter grid search to find the best hyperparameters using the Bayesian TPE sampler from Optuna~\cite{optuna} to effectively search the hyperparameter space. The hyperparameter values are detailed in Table~\ref{tab:hyperparameters}. The model which performs best on macro F1 in the validation split is chosen for the evaluation.

\input{tables/hyperparameters.tex}

% Evaluation
Finally, we evaluate the performance of the fine-tuned model on the held-out 70\% test set from the \texttt{crowdsourced} dataset in an unbalanced multi-label classification setting, focus on the macro F1 score to evaluate the overall performance of the model.
