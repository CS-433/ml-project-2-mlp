\section{Background}\label{sec:background}

\textbf{Homepage2Vec} addresses the limitations of existing approaches to web page representation by introducing a multilingual, embeddings-based model. Prior to its development, no multilingual models or widely adopted embedding-based methods existed for web page analysis, often necessitating the use of paid services. Homepage2Vec revolutionizes this landscape by providing a multilingual, open-source solution based on embeddings. Additionally, the model is efficient, enabling local execution without external APIs, ensuring accessibility and speed for a diverse range of users.

\texttt{Homepage2Vec} is trained on a publicly available website directory and corresponding labels called Curlie, maintained by a volunteer community. The directory comprises 3 million websites in 92 languages, labeled in hierarchical categories. After removing duplicates and retaining accessible sites, the dataset contains 886K entries. For classification, only top-level labels are considered, resulting in 14 classes.  The label distribution is imbalanced, with most websites categorized as Business (27\%), followed by Society (13.9\%) and Arts (9\%). The dataset is primarily single-labeled, with only 2.1\% of samples appearing in two or more taxonomy trees of the 14 top-level classes \cite{homepage2vec}.
% The majority of websites are in English (40\%), followed by German (16\%), French (5\%), and Japanese (6\%).

The model, after scraping a website's homepage, parses the raw HTML and URL into a one-dimensional embedding. Specifically, \textit{url}, \textit{title}, \textit{description}, \textit{keywords}, \textit{links}, and \textit{sentences} are embedded via the multilingual model XLM-R \cite{xmlr}, while \textit{tld} and \textit{metatags} are one-hot encoded based on a predefined list of the most common top-level domains (excluding regional ones) and meta-tags. For features resulting in multiple embeddings, such as sentences, the mean is taken. Finally, all embeddings are concatenated, resulting in an input dimension of $4665$. Optionally, a screenshot of the website can be used, embedded via ResNet-18 \cite{resnet}. The resulting embedding is then fed into a fully connected neural network with 2 hidden layers of sizes 1000 and 100, respectively, each followed by a ReLU activation function and dropout with a probability of $0.5$. Importantly, the outputs are treated separately, with each output transformed via sigmoid and interpreted as a probability for the given class.

Homepage2Vec, evaluated against an unbalanced Curlie test set, achieves a macro-averaged precision of $0.771$, recall of $0.549$, and an F1-score of $0.634$ \cite{homepage2vec}. While these results are promising, it is crucial to note that the majority of websites have at most one label. Therefore, the task is inherently easier than the one being addressed. To account for this, we evaluate the model on a crowdsourced dataset of 840 samples, where each website has an average of $2.5$ labels. We obtain a macro F1-score of $0.38$, serving as our baseline for further improvement using the proposed methods described in the following section.


% - GPT annotations 
\textbf{GPT based labeling.} The utilization of GPT for labeling purposes emerges as a powerful approach for streamlined annotation. The autonomous capability of GPT to propose examples to itself is exemplified in \cite{prompt-tuning}. It is noteworthy that the efficacy of GPT-based labeling demonstrates an upward trajectory with an increased number of examples, as evidenced in \cite{reduce-labeling-cost}. However, it is essential to acknowledge the associated cost escalation. AnnoLM contributes a holistic framework for GPT-based annotations, shedding light on the noteworthy observation that the inclusion of GPT-generated reasoning behind label assignments enhances overall performance, as elucidated in \cite{annollm}. 
Last but not the least, while GPT-based labeling has many advantages, relying on it during inference is not ideal due to the cost as well as the time. For this reason, as the previous works show \cite{reduce-labeling-cost,is-gpt3-good-annot}, much viable option is to use GPT generated labels to train a classifier that can be used for inference, which is the approach we take in this work.