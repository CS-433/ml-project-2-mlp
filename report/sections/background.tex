% Some important notes from the homepage2vec paper
% 1) Intro
% Before homepage2vec
% - No multilingual models
% - No embeddings based methods
% - Usually paid services
% With Homepage2vec
% - multilingual -> great because out of top 10M websites, 40% are not in English
% - embeddings based
% - open source
% - fast since you can run it locally, and do not have to use external API

% 2) Related work
% Related approaches
% - manual approaches back in the days
% - ML based methods that use contextual features of the given webpage
% - Methods that also use for context the surrounding webpages, especially useful when the current page does not include that much info
% - Methods that use vision features
% - Recently, use of the LSTM, BERT, GRU architectures -> shown to increase performance -> however focus purely on English

% Multilingual embeddings
% - They are using XML-R, multi-lingual model, that has shown to be comparable to the monolingual models

% 3) Dataset
% - Curlie = community edited web directory -> 3M websites in 92 languages,
% labeled in hierarchical categories, however they only used the top level categories
% Originally, there were 15 top level categories, but they dropped "Regional"
% - Majority of classes associated with Bussiness (27), Society (13.9) or Arts (9)
% - 40% of the websites are in English, 16 % in German, 5% in french, 6% in Japanese
% - Although each page may, in principle, have an arbitrary number of category labels, 
% at the top level, the data is mostly single-labeled, with only 2.1% of samples appearing 
% in two or more taxonomy trees of the 14 top-level classes.

% 4) Method
% - They embeded only the first 100 sentences since the embedding process is quite expensive
% This was selected based on the validation set performance using the elbow method
% - They use 19 most frequent domains exluding the domains that indicate country
% - Title, description and keywords are used as well and should be very informative
% - With the links, they use the anchor text and the 50 most frequent texts are used, again
% this was selected using the elbow method

% 5) Main limitations and challenges of homepage2vec
% 


% Background:
% - Homepage2Vec: Motivation, model, results + limitations (mismatch in label distribution between training and true)
% - GPT annotations (references from literature <- for ludek)
\section{background}\label{sec:background}
\textbf{Homepage2Vec} addresses the limitations in existing approaches to web page representation by introducing a multilingual, embeddings-based model. Prior to its development, there were no multilingual models or widely adopted embedding-based methods for web page analysis. Often, web page analysis relied on paid services. Homepage2Vec revolutionizes this landscape by offering a multilingual, open-source solution based on embeddings. Additionally, the model is efficient, allowing for local execution without the need for external APIs, making it accessible and fast for a diverse range of users.

\texttt{Homepage2Vec} is trained on a publicly available directory of websites and corresponding labels called Curlie, maintained by a volunteer community. The directory contains in total 3 million websites in 92 languages, labeled in hierarchical categories. After removing duplicate sites and keeping only the ones that can be accessed, the dataset contained 886K entries. Further, for the classification purposes, only the top-level labels were kept, resulting in 14 classes. The majority of the websites are in English (40\%), followed by German (16\%), French (5\%) and Japanese (6\%). The distribution of the labels is imbalanced, with the majority of the websites being labeled as Business (27\%), followed by Society (13.9\%) and Arts (9\%). The dataset is mostly single-labeled, with only 2.1\% of samples appearing in two or more taxonomy trees of the 14 top-level classes \cite{homepage2vec}.

After scraping the website's homepage, the model parses the raw html and url into one dimensional embedding. Specifically, \textit{url}, \textit{title}, \textit{description}, \textit{keywords}, \textit{links} and \textit{sentences} are embeded via multilingual model XLM-R \cite{xmlr}, while \textit{tld} and \textit{metatags} are one-hot encoded based on the predefined list of most common top-level domains (exluding regional ones) and meta-tags. For the features that result in multiple embeddings, such as sentences, mean of these is taken. Finally, all the embeddings are concatendated resulting in an input dimension of $4665$. Optionally, it is also possible to use screenshot of the website which is embeded via ResNet-18 \cite{resnet}. The resulting embeding is then fed into a fully conected neural network with 2 hidden layers of sizes 1000 and 100 respectively, each followed ReLU activation function and dropout with probability of 0.5. 
