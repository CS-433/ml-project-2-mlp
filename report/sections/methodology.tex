\section{Methodology}\label{sec:methodology}

The overall goal of our work is to improve mutlilabel classification performance of the original Homepage2Vec model~\cite{homepage2vec}. We can divide our work into two main phases: (1) identifying the best-performing LLM annotator and (2) fine-tuning the baseline model on a dataset annotated by the best-performing LLM annotator. In the following, we describe the methodology for each phase in detail. We encourage refering to Figure~\ref{fig:train-overview} throughout this section for a high-level overview of our methodology.

\subsection*{Phase 1: Identifying an Optimal LLM Labeler}

\input{tables/labeler-params.tex}

% Labeler Setup (Variants)
In our study we only consider GPT models using the OpenAI API, mainly because it provides the convenient way to obtain state-of-the-art performance. However, in theory our methodology can be applied to any LLM. We consider a total of 12 GPT labelers by varying the model version, context, and whether we include an example annotation in the prompt. Table~\ref{tab:labeler-params} shows the parameters and decriptions of each variant. Each unique parameter combination makes up a unique labeler.

% Details about parameters and variants
The context defines the amount of information about the website that is provided to the model in the system prompt and during the annotation process, and is inspired by the feature importance reported in the original Homepage2Vec paper~\cite{homepage2vec}. \texttt{context1} only uses information about the \texttt{tld}, \texttt{domain} and \texttt{metatags}. \texttt{context2} adds the \texttt{title}, \texttt{description}, and \texttt{keywords}, and \texttt{context3} adds the first 100 sentences and 50 links scraped from the website. If specified, an example annotation is provided to guide the labeler's annotation behaviour. The system prompt is kept constant across all labelers and is shown in Appendix~\ref{app:prompt}. The user prompt is a simple JSON dump of the context provided about the website to classify.

% Obtaining labels
We obtain labels from all labelers by manually scraping, preprocessing the websites and finally querying the GPT labeler. The scraping and processing pipeline is kept identical to the one used in Homepage2Vec~\cite{homepage2vec} to allow for comparison of the GPT labelers to the \texttt{baseline}. Some websites could not be reached at the time of writing, limiting the evaluation of all annotators to 761 websites.

% Gold standard
To identify high-quality annotations, we use the dataset \texttt{crowdsourced} as our ground truth by computing the macro F1 score between the labels obtained from the GPT labelers and the labels provided by the human annotators. For the human annotations, we assign a category label if at least two of the threee annotators agree, resulting in an average of 2.5 labels per website. This majority vote is necessary, because we found that annotators disagreed on numerous occasions, as measured by an inter-annotator agreement of 0.2.

% Curlie-10k dataset
Finally, we plan to use the GPT-3.5 and GPT-4 annotator that finds the best trade-off between cost and quality to label a random subset of 10,000 websites from the Curlie website directory. We will refer to these datasets as \texttt{curlie-gpt3.5-10k} and \texttt{curlie-gpt4-10k} respectively. The datasets are used in the second phase of our study to fine-tune the baseline model.

\subsection*{Phase 2: Transferring Knowledge via Finetuning}

The goal of phase 2 is to enrich Homepage2Vec by fine-tuning it on the labels obtained in Phase 1.

% Training
Training is performed on the \texttt{curlie-gpt3.5-10k} and \texttt{curlie-gpt4-10k} dataset for a maximum of 100 epochs. We use a 30\% held-out validation split from the \texttt{crowdsourced} dataset to monitor the validation F1 score and stop training if no improvement is observed for 20 epochs. This is to prevent overfitting the LLM labels. We perform hyperparameter grid search to Bayesian TPE sampler from Optuna~\cite{optuna} for $\eta=20$ trials and $\tau=5$ startup trials to effectively search the hyperparameter space. The hyperparameter values are detailed in Table~\ref{tab:hyperparameters}. The model which performs best on macro F1 in the validation split is chosen for the evaluation.

\input{tables/hyperparameters.tex}

% Evaluation
Finally, we evaluate the performance of the fine-tuned model on the held-out 70\% test set from the \texttt{crowdsourced} dataset in an unbalanced multi-label classification setting, focus on the macro F1 score to evaluate the overall performance of the model.
