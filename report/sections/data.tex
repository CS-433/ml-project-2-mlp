\section{Data}\label{sec:data}

\textbf{Original Data.} We use the crowdsourced Curlie dataset from Homepage2vec \cite{homepage2vec}, comprising 840 websites. Three annotators assigned top-level categories to each site, and we measured inter-annotator agreement using pairwise Cohen's kappa \cite{cohen-coef}. The mean pairwise Cohen's kappa is $0.2 \pm 0.02$, indicating low agreement. We assign a category label if at least 2 annotators agree, resulting in an average of $2.5$ labels per website. We then scrape and parse HTML content, extracting features such as top-level domain, domain, title, description, keywords, first 50 links, and first 100 sentences.

\input{tables/feature_information.tex}

\textbf{Curlie-gpt-10k.} We employ the best-performing GPT annotator evaluated against the human annotated original crowdsourced data to annotate \texttt{curlie-gpt-10k}, a dataset with 10k randomly selected websites from Curlie. Following the same preprocessing and feature extraction as the original dataset, Table \ref{tab:feature_information} displays feature percentages across datasets. As the figure clearly indicates, the most useful features according to the homepage2vec \cite{homepage2vec} descriptions and keywords are missing in around 55 \% and 75 \% of cases, respectively. 

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\columnwidth]{figures/category_distribution.png}
    \caption{Class distribution of the original dataset.}
    \label{fig:class_distribution}
\end{figure}

Figure \ref{fig:class_distribution} illustrates the class distribution of the original dataset. It is imbalanced, with most websites falling into the \textit{Business} category, followed by \textit{Arts} and \textit{Society}. This imbalance, identified in Homepage2vec, negatively impacted model performance on minority classes. We will address this issue in our Methods section.