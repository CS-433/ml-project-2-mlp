\section{Results}

\subsection*{Phase 1: Identifying an Optimal LLM Labeler}

Table~\ref{tab:labeler-results} shows the results of re-labelling \texttt{crowdsourced} dataset.

% General Results: Consistency, Cost, Quality
Our findings demonstrate that LLM labelers can provide \textit{consistent}, \textit{cost-effective}, and \textit{high-quality} annotations for the complex task of multilingual, multilabel website topic classification. 

% Consistency
Remarkably, not a single incorrect output was produced, underscoring the reliability the models in annotating websites.

% Cost
In terms of cost, the labeling of the \texttt{crowdsourced} corpus cost approximately \$130 per 1000 pages. Our approach, utilising GPT-3.5 and GPT-4 labelers, drastically reduces this cost to an average of \$0.54 and \$6.44, respectively, achieving a reduction by factors of 240x and 20x.

% Calculations
% Human annotator cost: 327 USD
% Pages annotated: 840 * 3 = 2520
% Cost per 1k page: 1000 * 327 / 2520 = 130$

% GPT-3.5 labler cost/1k pages:
% (0.36 + 0.48 + 0.42 + 0.63 + 0.57 + 0.80) / 6 = 0.54

% GPT-4 labler cost/1k pages:
% (4.68 + 5.75 + 5.26 + 7.25 + 6.68 + 8.99) / 6 = 6.44

% Cost reductions:
% GPT-3.5: 130 / 0.54 = 240x
% GPT-4: 130 / 6.44 = 20x

% Performance
Performance-wise, the best labeler, GPT-4 with \texttt{context3} and \texttt{1-shot}, achieves a macro F1 score of 46\% compared to the human annotations on the same dataset. Thus, the GPT labelers are better website classifiers than the baseline Homepage2Vec model, which achieves a macro F1 score of 39\% on the same dataset. This improvement gives us reason to believe that Homepage2Vec can learn from knowledge of the LLM labelers - the goal of the second phase of our study.

\input{tables/labeler-results.tex}

% GPT labeler parameter grid
\textbf{Labeler Parameter Grid.} Figure~\ref{fig:labelers-grid} visualises the effect of the labeler parameters on the annotation quality. As expected, we find that the quality of the labels increases with the amount of context provided and the complexity of the model used. Interestingly, the added features in \texttt{context3} (links and text) do not increase the annotation quality on average.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=.8\columnwidth]{figures/labeler-grid.pdf}
    \caption{\textbf{Labeler Parameter Grid.} The Figure displays the mean macro F1 score for all unique parameter combinations of the LLM labelers. For example, the top-right cell shows the average macro F1 score for all labelers that use GPT-3.5 with \texttt{1-shot} across all contexts.}
    \label{fig:labelers-grid}
\end{figure}

% Cost-quality trade-of
\textbf{Cost-Quality Trade-Off:} Our analysis reveals a positve trend between label quality and cost, attributable to the use of longer prompts or more sophisticated models. In the next phase, we aimed to select two labelers, one per model. In case of marginal improvements in label quality, we opted for the cheaper labeler. 
The best balance was achieved using \texttt{context2}; the GPT-3.5 labeler employed \texttt{1-shot}, whereas the GPT-4 used \texttt{0-shot}.

% Curlie-10k dataset
\textbf{Curlie-10k Dataset.} 
The average number of topics assigned to a page by the GPT 3.5 labeler is \textbf{1.6} and \textit{TODO} for the GPT-4 labeler, which is higher than \textbf{1.07} for the original Curlie dataset. Figure~\ref{fig:curlie-10k-dist} shows the distribution of the labels in the re-labelled dataset compared to the original. The average number of labels increased for every class.


\begin{figure}[!ht]
    \centering
    \includegraphics[width=.8\columnwidth]{figures/curlie-10k-dist.pdf}
    \caption{\textbf{Curlie-10k Label Distribution.} Topic distribution of \texttt{curlie-gpt3.5-10k}, \texttt{curlie-gpt4-10k}, as well as \texttt{curlie} for reference.}
    \label{fig:curlie-10k-dist}
\end{figure}


\subsection*{Phase 2: Knowledge Distillation}

% TODO: Would be cool to compare the labelling statistics of the exact 10k subsplit (Curlie vs. GPT) -- for now we will proxy.
In this section we provide the results of the second phase of our study, where we transfer the knowledge from the LLM labelers into the pre-trained Homepage2Vec model.

% Fine-tuning results
\textbf{Finetuning.} Table~\ref{tab:finetune-results} shows the results of the fine-tuning experiments. 
We observe that the fine-tuned model increases the recall significantly from 39.4\% to 47.6\%, at the cost of a minor decrease in precision from 40.9\% to 40.2\%. 
This increases the overall macro F1 score from 39.2\% to 42.6\%, which is a 9\% improvement. 
We have shown that the approach of fine-tuning the pre-trained model with the LLM labels can improve the performance on the texttt{crowdsourced} dataset that better resembles the true website topic classification.

% 0.391610 = 39.2% (Pre-trained Homepage2Vec)
% 0.426289 = 42.6% (GPT-3.5)
% Absolute Difference: 0.034679 = 3.5 percentage points
% Relative Difference: 0.086 = 8.6%

\input{tables/finetune-results.tex}

Figure~\ref{fig:finetune-results} shows the class-wise F1 score for the pre-trained model and the fine-tuned model. We observe that the fine-tuned model consistently outperforms the pre-trained model, achieving higher F1 scores in ten out of the 14 classes.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{./figures/exp2-mf1.pdf}
    \caption{\textbf{Finetune Results.} Class-wise F1 score for the pre-trained model and the fine-tuned model on te original crowdsourced data.}
    \label{fig:finetune-results}
\end{figure*}
