"""
Module to extract textual features from the html content of a webpage.

Includes:
    - TextualExtractor: Class to extract textual features from the html content of a webpage.
    - embed_text: Function to embed the text of a webpage.
    - embed_description: Function to embed the description of a webpage.
    - embed_keywords: Function to embed the keywords of a webpage.
    - embed_title: Function to embed the title of a webpage.
    - embed_links: Function to embed the links of a webpage.
    - embed_url: Function to embed the url of a webpage.
    - embed_tld: Function to embed the top-level domain of a webpage.
    - embed_metatags: Function to embed the metatags of a webpage.
    - split_in_sentences: Function to split the text of a webpage in sentences.
    - clean_url: Function to clean the url of a webpage.
    - clean_field: Function to clean a field of a webpage.
    - clean_link: Function to clean a link of a webpage.
    - trunc: Function to truncate the output of a tokenizer to a given length.
"""

import re
from collections import Counter

from bs4 import BeautifulSoup
from sentence_transformers import SentenceTransformer


class TextualExtractor:
    """
    Extract textual features from the html content of a webpage
    """

    xlmr = None

    def __init__(self, device="cpu"):
        if not TextualExtractor.xlmr:
            TextualExtractor.xlmr = SentenceTransformer(
                "paraphrase-xlm-r-multilingual-v1", device=device
            )
        # self.xlmr = SentenceTransformer('xlm-r-distilroberta-base-paraphrase-v1', device=device)

        # TLD used for one-hot encoding
        self.rep_tld = [
            "com",
            "org",
            "net",
            "info",
            "xyz",
            "club",
            "biz",
            "top",
            "edu",
            "online",
            "pro",
            "site",
            "vip",
            "icu",
            "buzz",
            "app",
            "asia",
            "su",
            "gov",
            "space",
        ]

        # Metatags used for one-hot encoding
        self.rep_metatags = [
            "viewport",
            "description",
            "generator",
            "keywords",
            "robots",
            "twitter:card",
            "msapplication-tileimage",
            "google-site-verification",
            "author",
            "twitter:title",
            "twitter:description",
            "theme-color",
            "twitter:image",
            "twitter:site",
            "format-detection",
            "msapplication-tilecolor",
            "copyright",
            "twitter:data1",
            "twitter:label1",
            "revisit-after",
            "apple-mobile-web-app-capable",
            "handheldfriendly",
            "language",
            "msvalidate.01",
            "twitter:url",
            "title",
            "mobileoptimized",
            "twitter:creator",
            "skype_toolbar",
            "rating",
        ]

        # number of sentences and links over which we compute the features
        self.k_sentences = 100
        self.k_links = 50

    def get_features(self, url, html):
        features = {}

        # url
        url_feature = embed_url(url, TextualExtractor.xlmr)
        features["f_url"] = url_feature

        # tld
        tld_feature = embed_tld(url, self.rep_tld)
        features["f_tld"] = tld_feature

        # print(html)
        soup = BeautifulSoup(html, "lxml")

        # metatags
        metatags_feature = embed_metatags(soup, self.rep_metatags)
        features["f_metatags"] = metatags_feature

        # title
        title_feature = embed_title(soup, TextualExtractor.xlmr)
        features["f_title"] = title_feature

        # description
        description_feature = embed_description(soup, TextualExtractor.xlmr)
        features["f_description"] = description_feature

        # keywords
        keywords_feature = embed_keywords(soup, TextualExtractor.xlmr)
        features["f_keywords"] = keywords_feature

        # links
        links_feature = embed_links(soup, TextualExtractor.xlmr, self.k_links)
        features["f_links_" + str(self.k_links)] = links_feature

        # text
        text_feature = embed_text(soup, TextualExtractor.xlmr, self.k_sentences)
        features["f_text_" + str(self.k_sentences)] = text_feature

        return features


def embed_text(soup, transformer, k_sentences):
    """Embed the text of a webpage""" ""
    sentences = split_in_sentences(soup)[:k_sentences]

    if len(sentences) == 0:
        return None

    # this is needed to avoid some warnings, truncate the sentences
    sentences_trunc = [
        trunc(s, transformer.tokenizer, transformer.max_seq_length) for s in sentences
    ]

    sentences_emb = transformer.encode(sentences_trunc)

    if sentences_emb.size == 0:
        return None

    text_emb = sentences_emb.mean(axis=0).tolist()  # mean of the sentences

    return text_emb


def embed_description(soup, transformer):
    """Embed the description of a webpage""" ""
    desc = soup.find("meta", attrs={"name": ["description", "Description"]})

    if not desc:
        return None

    content = desc.get("content", "")

    if len(content.strip()) == 0:
        return None

    content = clean_field(content)

    # this is needed to avoid some warnings
    desc_trunc = trunc(content, transformer.tokenizer, transformer.max_seq_length)
    desc_emb = transformer.encode(desc_trunc)

    if desc_emb.size == 0:
        return None

    return desc_emb.tolist()


def embed_keywords(soup, transformer):
    """Embed the keywords of a webpage""" ""
    kw = soup.find("meta", attrs={"name": "keywords"})

    if not kw:
        return None

    content = kw.get("content", "")

    if len(content.strip()) == 0:
        return None

    # this is needed to avoid some warnings
    kw_trunc = trunc(content, transformer.tokenizer, transformer.max_seq_length)
    kw_emb = transformer.encode(kw_trunc)

    if kw_emb.size == 0:
        return None

    return kw_emb.tolist()


def embed_title(soup, transformer):
    """Embed the title of a webpage""" ""
    title = soup.find("title")

    if title is None:
        return None

    title = str(title.string)
    title = clean_field(title)

    if len(title) == 0:
        return None

    # this is needed to avoid some warnings
    title_trunc = trunc(title, transformer.tokenizer, transformer.max_seq_length)
    title_emb = transformer.encode(title_trunc)

    if title_emb.size == 0:
        return None

    return title_emb.tolist()


def embed_links(soup, transformer, k_links):
    """Embed the links of a webpage""" ""
    a_tags = soup.find_all("a", href=True)

    links = [a.get("href", "") for a in a_tags]
    links = [clean_link(link) for link in links]
    links = [link for link in links if len(link) != 0]

    words = [w.lower() for w in " ".join(links).split(" ") if len(w) != 0]

    if len(words) == 0:
        return None

    most_frequent_words = [w[0] for w in Counter(words).most_common(k_links)]

    # most_frequent_words = pd.Series(words).value_counts()[:k_links].index.values

    # this is needed to avoid some warnings
    words_trunc = [
        trunc(w, transformer.tokenizer, transformer.max_seq_length)
        for w in most_frequent_words
    ]
    words_emb = transformer.encode(words_trunc)

    if words_emb.size == 0:
        return None

    links_emb = words_emb.mean(axis=0).tolist()

    return links_emb


def embed_url(url, transformer):
    """Embed the url of a webpage"""
    cleaned_url = clean_url(url)

    url_emb = transformer.encode(cleaned_url)

    if url_emb.size == 0:
        return None

    return url_emb.mean(axis=0).tolist()


def embed_tld(url, rep_tld):
    """Embed the top-level domain of a webpage"""
    tld = url.split(".")[-1]
    rep_onehot = [int(tld.startswith(d)) for d in rep_tld]
    continent_onehot = 7 * [0]  # TODO

    return rep_onehot + continent_onehot


def embed_metatags(soup, rep_metatags):
    """Embed the metatags of a webpage"""
    metatags = soup.findAll("meta")
    attr = [m.get("name", None) for m in metatags]
    attr = [a.lower() for a in attr if a is not None]

    attr_emb = [int(a in attr) for a in rep_metatags]

    return attr_emb


def split_in_sentences(soup):
    """From the raw html content of a website, extract the text visible to the user and splits it in sentences"""

    sep = soup.get_text("[SEP]").split(
        "[SEP]"
    )  # separate text elements with special separators [SEP]
    strip = [s.strip() for s in sep if s != "\n"]
    clean = [s for s in strip if len(s) != 0]

    return clean


def clean_url(url):
    """Clean the url of a webpage"""
    url = re.sub(r"www.|http://|https://|-|_", "", url)
    return url.split(".")[:-1]


def clean_field(field):
    """Clean a field of a webpage"""
    field = re.sub(r"\*|\n|\r|\t|\||:|-|â€“", "", field)
    return field.strip()


def clean_link(link):
    """Clean a link of a webpage"""
    link = re.sub(r"www.|http://|https://|[0-9]+", "", link)
    link = re.sub(r"-|_|=|\?|:", " ", link)
    link = link.split("/")[1:]
    return " ".join(link).strip()


def trunc(seq, tok, max_length):
    """Truncate the output of a tokenizer to a given length, doesn't affect the performances"""
    e = tok.encode(seq, truncation=True)
    d = tok.decode(e[1:-1][: max_length - 2])
    return d
